\documentclass[greybox]{svmult}

\usepackage{mathptmx}       % selects Times Roman as basic font
\usepackage{helvet}         % selects Helvetica as sans-serif font
\usepackage{courier}        % selects Courier as typewriter font
\usepackage{type1cm}        % activate if the above 3 fonts are


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cancel}
\usepackage{allrunes}
\usepackage{graphics}
\usepackage{verbatim}
\usepackage{color}
\usepackage{tikz}
\usepackage[curve]{xypic}
\usetikzlibrary{arrows}
\usepackage{algorithm2e}

\input{acros.tex}

\newcommand{\noo}[1]{}
\renewcommand{\bar}[1]{\overline{#1}}
\newcommand{\outa}[2]{#1^+(#2)}
\newcommand{\ina}[2]{#1^-(#2)}
\newcommand{\neu}[2]{#1^n(#2)}
\newcommand{\proto}{\mathbf C}
\newcommand{\clab}{\mathsf c}
\newcommand{\iterate}[1]{{\cal I}(#1)}
\newcommand{\cl}{cl}
\newcommand{\dstate}[1]{\mathsf D({#1}) }
\newcommand{\witness}[1]{{\mathsf w^{#1}}}
\newcommand{\buffer}[2]{{\mathsf b}_{#1}(#2)}
\newcommand{\sdepth}[2]{d_{#1}(#2)}
\newcommand{\shrink}[2]{\rho_{#1}(#2)}
\newcommand{\comp}[2]{C(#1,#2)}
\newcommand{\test}[1]{{#1}?}
\newcommand{\dlangm}{{\mathcal L}_{\acro{DDL}}}
\newcommand{\pis}[1]{{\mathbf e}_{#1}}
\newcommand{\carriers}[1]{Q_{#1}}
\newcommand{\kmod}[2]{{\cal K}_{(#1,#2)}}
\newcommand{\rels}[1]{{\mathsf R_{#1}}}
\newcommand{\update}[3]{{\mathcal U}_{#1}(#2,#3)}
\newcommand{\cons}[1]{{\textara{\ea}}(#1)}
\newcommand{\af}{{\mathsf F}}
\newcommand{\afn}{S}
\newcommand{\afe}{E}
\newcommand{\seq}[1]{\overrightarrow{#1}}
\newcommand{\basis}{basis }
\newcommand{\state}{state }
\newcommand{\views}{\mathcal B}
\newcommand{\viewsv}{\left(V_a\right)_{(a \in \agents)}}
\newcommand{\carrier}{Q_\views}
\newcommand{\sem}{\varepsilon}
\newcommand{\depth}[1]{|{#1}|^\adia}
\newcommand{\bisim}{\underline{\leftrightarrow}}

\begin{document}
\title*{Arguably argumentative: A formal approach to the argumentative theory of reason}
\author{Sjur K. Dyrkolbotn and Truls Pedersen}
\institute{Sjur K. Dyrkolbotn \at Durham Law School, Durham University, \email{s.k.dyrkolbotn@durham.ac.uk}
\and Truls Pedersen \at Department of Information Science and Media Studies, University of Bergen}

\maketitle

\abstract{
We propose a formal approach to the argumentative theory of reason, combining argumentation theory and modal logic in a novel way. We argue that the resulting framework can be used to model important mechanisms identified by the theory, including how confirmation bias and other modes of reasoning typically considered fallacious may in fact serve an important argumentative purpose that can also give rise to classically sound conclusions through the process of social deliberation. We go on to suggest that the argumentative theory is based on an understanding of intelligent reasoning and rationality that sees these notions as irreducibly social, and that the argumentative theory itself provides a possible starting point in the search for new theoretical foundations based on this understanding. Moreover, we suggest that formal logic can be used also as a means to investigate foundational issues, and we sketch the development of an axiomatic approach to the study of rational deliberative interaction.}

\section{Introduction}\label{sec:intro}

The idea that social interaction and rationality are mutual dependent notions is becoming increasingly important in many different fields of research, including economy, law, biology and artificial intelligence \cite{blume,dworkin,waal,benthem,ossowski}. In all these research areas, there is a trend towards  viewing rationality as fundamentally embedded in a social context. Importantly, this context is seen as important not only because people are social and tend to interact, but also because \emph{who} they are, \emph{what} they want, and \emph{why} they want it, tends to depend on how they engage with each other and their environment.\footnote{This perspective has long been adopted in social psychology, particularly in the tradition going back to George Herbert Mead and the Chicago school \cite{mead}. But to many other fields, particularly those developed following the pardigm of rational choice theory, it represents an important recent trend.}
 
To accommodate this point of view across different domains, we are in need of better theoretical foundations, allowing us to investigate the relationship between reasoning and interaction, taking into account that they are co-dependent and co-evolving. In this paper, we argue that this challenge can be addressed using formal logic, drawing on tools and techniques developed in the context of multi-agent systems.\footnote{The connection between various branches of social science and formal logic and computer science has received much attention in recent years, and it has led to a surge of interest in interdisciplinary research \cite{parikh,benthem2,verbrugge}. However, while much recent work in applied logic has been devoted to modeling agency and interaction, the usual starting point is still that agents reason in adherence to some standards of correctness, specified by formal logic. Also, it is typically assumed that rational interaction emerges from the fact that agents are individually rational in some appropriate sense, for instance because they seek to maximize given utility functions. 

In this paper we will argue that in order to provide adequate formal foundations for rational interaction we must depart from such reductionist assumptions.} In particular, we propose a formal approach to the argumentative theory of reason, introduced in \cite{mercier}. The key idea there is that reasoning evolved to facilitate efficient argumentation, and not necessarily to help us arrive at logically correct forms of inference, or in making reasonable decisions.

This can have important implications for logical modeling of rational interaction, and we sketch the development of a general logical framework that enables us to capture the idea formally. Our overreaching aim is to argue that a formal approach to the argumentative theory of reason can provide interesting new insights, particularly regarding the social nature of rationality. 

The structure of the paper is as follows. In Section \ref{sec:arg} we present the argumentative theory, focusing on its implications for our understanding of agency. We argue that the notion of argumentation that is at work challenges existing traditions in argumentation theory, particularly formal theories, and we go on to present an alternative formalization which looks at argumentative structures in a new light, as the basis upon which agents' subjective interpretations of the world are formed. In short, we introduce the \emph{argumentative agent}, and we argue that he should be studied further.

In Section \ref{sec:ddl} we go on to present a logical formalism for studying argumentative deliberation, interaction between argumentative agents that serve to generate novel interpretations of the world. Since our purpose in this paper is to focus on main ideas rather than technical details, we present a very simple formal framework, containing only some very basic constructions which can be developed further using existing tools from modal logic. We show through examples that this is sufficient to allow us to capture essential aspects of the mechanisms addressed in \cite{mercier}, and we also sketch the development of an axiomatic approach to deliberative rationality. In Section \ref{sec:conc} we discuss future work and offer a conclusion.

\section{Argumentative agents: A semantics for individual reasoning based on argumentation}\label{sec:arg}

The argumentative theory of reason is formulated on the basis of experimental evidence which appears to suggest that human reasoning evolved to facilitate efficient argumentation. From the point of view of an individual, it seems that the most significant purpose of reason is not in helping him to arrive at logically correct forms of inference, but to maximize his chance of winning arguments. This, according to the theory, explains why humans so often reason in a way that is regarded as fallacious. According to the argumentative theory, reasoners sometimes commit fallacies because they are useful to them in the context of argumentation. The most obvious example is confirmation bias, the tendency to look disproportionately for reasons to support existing beliefs. It is only natural that agents who seek to win arguments will tend to focus on looking for reasons to support their own positions, rather than looking for reasons to reject them.

More generally, the argumentative theory suggests that in order to model reasoning in a descriptively accurate way, we must take into account that agents are argumentative. In terms of rational choice terminology, one might express this by saying that agents' utility functions are heavily influenced by their desire to do well when they argue with others, often to the detriment of sound classical reasoning.

This insight is interesting in itself, but it is also connected to a second insight stemming from the argumentative theory, regarding the conceptual foundation for our understanding of intelligence and rationality. 
We believe, in particular, that the argumentative theory suggests looking for rationality principles that do not target individual reasoners at all, but rather the deliberative processes that they partake in. It is striking, as pointed out in \cite{mercier}, how deliberation can often lead to classically sound outcomes even if each individual reasoner is argumentative or unreasonable. We follow \cite{mercier} in thinking that this mechanism is crucial, and that it can also serve to explain why argumentative reasoning has proved so successful for the human race, even if it regularly leads to unsound, or even absurd, results, when people reason in isolation. 

To develop a formal account of argumentative deliberation we will start from a formal representation of the reasoners themselves. In this regard, it is important to note that the argumentative theory involves a notion of argumentation which is conceptually distinct from that found in traditional argumentation theory, as it has been developed in modern times following the influential work of \cite{toulmin} (first edition from 1958). In this research tradition, theories  tend to be highly normative, focusing on recognizing and categorizing fallacies and on designing argumentation schemes and models that are meant to facilitate sound and rational reasoning, particularly regarding what arguments we should accept in a given scenario. The argumentative theory, on the other hand, asks us to look at human reasoning at the individual level more descriptively, and to take seriously the fact that reasoning appears to have evolved as a mechanism to facilitate \emph{efficient} argumentation, and might not conform to any general standards of correctness.

How can we formalize the argumentative agent, when argumentation is understood in this way? In the following we will use argumentation frameworks, first introduced in \cite{dung}. These are simple mathematical objects, essentially directed graphs, which facilitate the investigation of a whole range of semantics \cite{baroni}.\footnote{The theory of argumentation frameworks has been influential in the context of artificial intelligence \cite{rahwan}. It is capable of capturing many different semantic notions, including semantics for multi-valued and non-monotonic logics, logic programs and games \cite{dung,dyrkolbotn}. The work of \cite{brewka}, on the other hand, shows how argumentation frameworks can be used to provide a faithful (and computationally efficient) representation also of semantics that are formulated with respect to the more fine-grained formalism of abstract dialectical frameworks \cite{brewka1}. It is also important to note that much recent work focuses on providing logical foundations for the theory \cite{grossi,grossi1,arieli,caminada}, which we can draw on when we develop multi-agent extensions.} We think they are well suited as a technical starting point towards logics for argumentative deliberation, but we propose to make use of them in a novel way.

We will not use them to model concrete argumentation scenarios, but to represent the agents' interpretations of semantic meaning.\footnote{In terms of each individual agent, using terminology from cognitive science, this places the argumentation framework at the informational level of cognitive processing, where previous work have already shown that logical tools can have a particularly crucial role to play, also serving to shed new light on established truths arrived at through empirical work, see e.g., \cite{stenning}.} This makes good sense with respect to the argumentative theory; Since agents reason to win arguments, it is natural to assume that they tend to represent semantic information in argumentative terms.

In addition to this, the theory of argumentation frameworks provide a flexible framework for modeling many different ways in which individual agents may choose to reason, using one among the many semantics that have been developed for reasoning about such frameworks. Hence we make no commitment to a given set of reasoning rules or principles -- the argumentative agent is defined by the fact that he maintains an argumentative interpretation of the world, not by the fact that he reasons about it in a given way. We now go on to present the formal details, before we consider an extended example.

%While much work on multi-agent argumentation has already been carried out in a formal and semi-formal context, we note that this work is mostly based on a traditional view of argumentation theory.\footnote{For instance, we think this is implicit in recent formal work such as that of \cite{pigozzi,pigozzi1} and even more so in the survey of the field given in \cite{rahwan}.} In our opinion, however, this view is inappropriate when attempting to formalize the argumentative theory. The problem is that the representation of the argumentation scenario is fixed and not open to dispute and dynamic change, except with respect to the question of how it should be evaluated. But to model the argumentative theory, we need to depart from this starting point, since it is crucial that the basic representation of the surrounding semantic reality is itself a subjective construction, distinctly produced in each individual agent. This is why we use argumentation frameworks as models of the agents' internal view of the relevant arguments and how they are related. We return to the dynamics of deliberation in Section \ref{sec:ddl}, but first we offer a technical presentation of argumentation frameworks, and how they can be used to define a logic for talking about the views of argumentative agents.

\subsection{Argumentation frameworks, agents and semantic views}\label{subsec:arg}

Given a set of semantic atoms $\Pi$, which we will tend to think of as names of arguments, an argumentation framework (AF) over $\Pi$ is a relation $E \subseteq \Pi \times \Pi$. Intuitively, an element $(x,y) \in E$ encodes the fact that arguments $x$ attacks argument $y$. We can depict $E$ as a directed graph, giving a nice visualization of how the atoms in $\Pi$ are related as arguments, see Figure \ref{fig:1} for an example. 
We introduce the notation $\outa E x = \{y \in \Pi \mid (x,y) \in E\}, \ina E x = \{y \in \Pi \mid (y,x) \in E\}$ and $\neu E x = \Pi \setminus \outa E x \cup \ina E x$. We use $\Pi(E) = \{x \in \Pi \mid \neu E x = \Pi\}$ to denote the set of atoms from $\Pi$ that does not appear in any attack from $E$.

\begin{figure}
$$
\xymatrix{p \ar@(lu,ld) \ar@/_/[r] & q \ar@/_/[l] \ar@/_/[r] & q' \ar@/_/[l] \ar@/_/[r] & p' \ar@(ru,rd) \ar@/_/[l] }
$$
\caption{An AF $E$ such that $\Pi(E) = \{p,q,q',p'\}$}
\label{fig:1}
\end{figure}

Given an AF $E$, the purpose of an argumentation semantics is to identify, in terms of $E$, the 
collection of sets of arguments that can be accepted if they are taken together, typically called \emph{extensions}, see e.g., \cite{wu}. For instance, if $E = \{(p,q),(r,p)\}$, then the semantics might prescribe $\{r,q\}$ as a set that can be accepted, since $r$ defends $q$ against the argument made by $p$ and $r$ is not in turn attacked. It is natural to represent semantics for AFs using three-valued assignments. In particular, given an AF $E$ and an extension $A \subseteq \Pi$, the associated three-valued assignment is $\clab_A:\Pi \to \three$ given by
$$
\clab_A(x) = \begin{cases} 1 \text{ if } x \in A \\ 0 \text{ if } x \in \outa E A \\ \frac{1}{2} \text{ otherwise } \end{cases}
$$
This representation corresponds to an intuitive reading where arguments in $A$ are regarded as true/successful, arguments attacked by one of these are regarded as false/failed, while all other arguments are taken to have an undecided semantic status. The three-valued representation of extensions also facilitate elegant definitions of various argumentation semantics, based on thinking of semantics as prescribing collections of three-valued assignments to AFs. In this way, it also becomes natural to reason about AFs using three-valued logic, an idea that has been explored in some recent work \cite{dyrkolbotn,arieli,dyrkolbotn1}. This will be exploited in the coming sections, as we will rely on three-valued {\L}ukasiewicz logic when we evaluate complex formulas over given AFs.

All semantics for argumentation of which we are aware are based on the notion of \emph{conflict-freeness}. Formally, an assignment $\clab$ is said to be conflict-free for an AF $E$ just in case the following holds, for all $x \in \Pi$.
\begin{equation}\label{eq:cam}
\begin{array}{l}
\clab(x) = 0 \iff \exists y \in E^-(x): \clab(y) = 1
\end{array}
\end{equation}
Given $E$ we let $\proto(E)$ be the set of conflict-free assignments for $E$, and we define $\clab^1 = \{x \in \Pi \mid \clab(x) = 1\}, \clab^0 = \{x \mid \clab(x) = 0\}$ and $\clab^{\frac{1}{2}} = \{x \in \Pi \mid \clab(x) = \frac{1}{2}\}$. In Figure \ref{fig:sem} we provide definitions of the most commonly known semantics based on argumentation frameworks, which are based on restricting $\proto(E)$. 

The logic introduced in the next section is parameterized by an argumentation semantics and the choice of such a semantics will not crucial be for our analysis in this paper. We note, however, that the \emph{admissible} semantics encode what seems to be minimal criteria of acceptability of arguments. Intuitively, it requires that an acceptable set must be free from internal conflict and that it must also be able to defend itself against all attacks. The other semantics in Figure \ref{fig:sem} are all based on the same idea, but adds other requirements that are less obviously reasonable. In the following we will let $\sem$ refer to some generic semantics defined in Figure \ref{fig:sem}.

\begin{figure}
$\begin{array}{ll}
\text{\footnotesize{Admissible: }} & a(E) = \{\clab \in \proto(E) \mid  E^-(\clab^1) \subseteq \clab^0\} \\
\text{\footnotesize{Complete:}} & c(E) =  \{\clab \in \proto(E) \mid \clab^1 = \{x \in \Pi \mid E^-(x) \subseteq \clab^0\}\} \\
%\forall x \in \Pi: \\ & \clab(x) = 1 \iff \forall y \in E^-(x): \clab(y) = 0\} \\
%\text{\footnotesize{Grounded:}} & g(E) = \{\bigcap c(E)\} \\
\text{\small{Preferred:}} \ \ & p(E) = \{\clab_1 \in a(E) \mid \forall \clab_2 \in a(E): \clab^1_1 \not \subset \clab^1_2\} \\
\text{\small{Semi-stable:}} \ \ & ss(E) = \{\clab_1 \in a(E) \mid \forall \clab_2 \in a(E): \clab^{\frac{1}{2}}_1 \not \supset \clab^{\frac{1}{2}}_2\} \\
\text{\small{Stable:}} \ \ & s(E) = \{\clab \in a(E) \mid \clab^{\frac{1}{2}} = \emptyset\} 
\end{array}$
\caption{Various semantics, defined for any $E \subseteq \Pi \times \Pi$}
\label{fig:sem}
\end{figure}

Towards a logic for reasoning about AFs we will start from the simple propositional language $\lang$, defined by the grammar below.

$$
\alpha := \ p \ \mid \ \neg \alpha \ \mid \ \alpha \to \alpha 
$$
where $p \in \Pi$. We can then define the inductive extension of three-valued assignments to arbitrary formulas from $\lang$ as in {\L}ukasiewicz three-valued logic, as detailed below.
\begin{equation}\label{eq:lsem}
\begin{array}{l}
\overline \clab(p) = \clab(p) \text{ for } p \in \Phi \\
\overline \clab(\neg \alpha) = 1 - \overline \clab(\alpha) \\
\overline \clab(\alpha \to \beta) = min\{1,1-(\overline \clab(\alpha) - \overline \clab(\beta))\}
\end{array}
\end{equation}

This evaluation behaves like classical logic on the semantic values $\{0,1\}$. For an intuition of how the third value is dealt with, notice that the definition ensures that if $\clab(p) = \frac{1}{2}$, we have $\overline \clab(p \leftrightarrow \neg p) = 1$. In particular, the claim that $p$ is equivalent with its own negation is true just in case $p$ has value $\frac{1}{2}$. This means, in particular, that the logic allows us to express that some argument, or collection of arguments , have this value. This is what makes it so natural to use {\L}ukasiewicz logic to evaluate complex formulas, as opposed to some other three-valued logic, such as Kleene's logic. 

We are now prepared to offer our definition of an argumentative agent. To this end, let $\agents$ be a set of agent names. Then a \emph{view} for agent $a \in \agents$ is an AF $V_a \subseteq \Pi \times \Pi$. It encodes his interpretation of the semantic relationship between the arguments under consideration. We also define an \emph{argumentative state} as a tuple $(V_a)_{a \in \agents}$, associating a view with each agent. In this paper, we will assume for simplicity that the argumentative state remains the same throughout the course of deliberation, so that the views of the agents are not themselves subject to revision as the debate unfolds. This, however, can be extended in future work, by nested application of the ideas we develop in the next sections.

Now, given an agent $a \in \agents$ with a view $V_a$, we can add the modality $\cdia_a$ to perform (boolean) meta-reasoning about the acceptance status of arguments on AFs, under some arbitrary semantics $\sem$. In particular, we get the following multi-agent language $\lblack$
$$
\phi := \cdia_a \alpha \ \mid \ \neg \phi \ \mid \ \phi \land \phi $$ where $\alpha \in \lang$ and $a \in \agents$. 

When we wish to reason about a single AF, we will omit reference to the single agent who instantiates it, and write simply $\cdia \alpha$. Given an argumentative state $\views = (V_a)_{a\in \agents}$, we can now define truth for formulas from $\lblack$ inductively as follows, for all formulas $\phi$
\begin{equation}\label{eq:asem}
\begin{array}{l}
\views \models_\sem \cdia_a \alpha \text{ if there is } \clab \in \sem(V_a) \text{ s.t. } \overline \clab(\alpha) = 1 \\
\views \models_\sem \neg \phi \text{ if not } \views \models_\sem \phi \\
\views \models_\sem \phi \land \psi \text{ if } \views \models_\sem \phi \text{ and } \views \models_\sem \psi 
\end{array}
\end{equation}

For instance, assume we have a single agent $a$ and that his view, $V_a$, is given by the AF in Figure \ref{fig:1}. Then it is easily verified that the following claims holds for all semantics $\sem$.
\begin{itemize}
\item $V_a \models_\sem \cdia q$
\item $V_a \models_\sem \neg \cdia p \land \cdia \neg p$
\item $V_a \models_\sem \cdia (\neg q \land q')$
\item $V_a \models_\sem \cbox (\neg q \to (p \leftrightarrow \neg p))$
\end{itemize}

Having access to a formal logical language for talking about individual views of agents, we can now explore formally what we mean by classically sound reasoning. We already noted that the evaluation of complex formulas agrees with classical logic on the boolean values. So if the argumentation semantics that the agent uses to reason only returns two-valued assignments, disallowing assignments that assign $\frac{1}{2}$ to some arguments, it might be tempting to say that he reasons classically. 

However, the core requirements underlying the semantics from Figure \ref{fig:sem} also seem to capture some crucial aspects of what we mean by classical soundness. First, and most importantly, they all require us to disallow assignments that assign $1$ to two arguments that are in conflict. This is intuitively speaking a classical soundness principle, an expression of the law of non-contradiction. However, if we accept this requirement as part of our notion of what it means to reason classically, we immediately encounter situations where the underlying interpretation of the semantic reality makes this impossible. For instance, there is no way to reason classically about an argument that attacks itself. Under all semantics disallowing the assignment of $1$ to two arguments in conflict such an argument must by necessity obtain the value $\frac{1}{2}$.

In terms of logic, we have $\cbox (x \leftrightarrow \neg x)$ expressing that $x$ is necessarily equivalent to its own negation. In this case, classical reasoning breaks down already due to a problematic argumentative interpretation of reality, not due to any shortcoming of the principles used to reason about it. In fact, there are strong formal reasons for thinking that classical reasoning about AFs is captured by the stable semantics \cite{dyrkolbotn}. Recall that the stable semantics collects all those labellings that are admissible and also satisfy the requirement that the set of arguments assigned $1$ attacks all other arguments, c.f., Figure \ref{fig:sem}. Hence it also always ensures that every argument is assigned a boolean value, and in this way its classical nature becomes apparent. If we now wish to consider separately the class of those argumentative states for which classical reasoning is possible, we can do so using a formal definition which takes this to be the class of states where each agents' views admits a non-empty set of stable labellings.

The necessary failure of classical reasoning in the context of certain problematic interpretations of reality can now be characterized in terms of graph structures for which no stable assignment exists. 
In fact, the combinatorial problem of when an AF admits a stable set has been analyzed in graph theory since the 50s, by researchers using a different terminology, in the field of \emph{kernel theory}. The core result from this field, due to \cite{richardson}, immediately implies that any AF which has no directed odd cycle of attack admits a non-empty set of stable assignments. This result was also rediscovered in \cite{dung}, but kernel theory offers many additional results and techniques, see for instance \cite{sanches}. These result can now all be understood as giving conditions which ensure the possibility of imposing classical standards of reasoning on agents' interpretations of the world.

%While formalizing the argumentative agent already opens up interesting new ways of looking at classical reasoning and rationality, the main challenge that remains is to find a way to formally introduce an appropriate kind of multi-agent interaction. The formal framework should allow us to study deliberative processes which are based on the agents' interpretations of the world, and which can result in aggregated views that are potentially different from all of them. One of the questions we can then address is the question of what principles we need, at the social level, to make sure that classical reasoning can be regained through deliberation. Before we move on to tackle this challenge, we give an extended example.

\subsection{Extended example: Rain in Bergen}\label{ex:run}

In this section, we consider in depth a simple example which motivates the need for introducing subjective views, suggesting also some shortcomings of a traditional approach to argumentation in the context of multi-agent deliberation. We assume given two agents $a,b$ who argue about whether it will rain in Bergen today, where $r$ represents the claim that it will rain, while $\overline r$ represents the claim that it will not. Let us first assume that none of the agents actually argue in favor of their positions. However, assuming that the disagreeing parties recognize that they disagree, the positions themselves play an argumentative role, and the AF shown on the left below represents the initial state of disagreement. So far, in particular, the model appears to be an uncontroversial objective representation of the state of affairs. There is not yet any discernible need for introducing subjective views. 
$$
\begin{array}{ccc}
\xymatrix{ r \ar@/_/[r] & \overline r \ar@/_/[l] } & \hspace{3em} &
\xymatrix{ r \ar@/_/[r]_{{ a},{ b}} & \bar r \ar@/_/[l]_{{ a},{ b}} }
\end{array}
$$
On the right above, we show a simple agent-indexed AF which illustrates a naive attempt at introducing agency to the initial AF. We label the two attacks, from $r$ to $\overline r$ and from $\overline r$ to $r$, by both $a$ and $b$ to encode that they are \emph{common} to the agents. That is, both agents acknowledge that these attacks are present -- they agree that they disagree.

In this case, the rational outcome, the position that \emph{should} be adopted, is unclear. It seems, in particular, that both $r$ and $\bar r$ are acceptable, since no further arguments have been made. This, indeed, also follows from the admissibility criterion in pure argumentation theory -- both $\{r\}$ and $\{\bar r\}$ (but not both) can be taken as true, for all semantics defined in Figure \ref{fig:sem}.

Assume that $a$ and $b$ begin to argue in favor of their claims. We keep it simple and consider only two steps of debate: first $a$ introduces the argument that the weather report says that it will not rain, and then $b$ counters this by announcing that he has seen a puddle on the pavement, suggesting that it has already been raining today. Let us call the arguments provided by the weather report and the puddle $w$ and $p$ respectively. Then, noting that $w$ is an argument used by $a$ against $r$ and that $p$ is an argument used by $b$ as a retort against $w$ and also, let us assume, directly against $\bar r$, a naive representation in the traditional spirit would be to view the debate as progressing from $(1)$ to $(3)$, as depicted in Figure \ref{fig:wrong}.
\begin{figure}
$$
\begin{array}{ccc}
(1): & (2): & (3): \\
\xymatrix{ \\ r\ar@/_/[r]_{a,b} & \bar{r}\ar@/_/[l]_{a,b} } &
\xymatrix{
        w\ar[d]_{a} & \\
        r\ar@/_/[r]_{a,b} & \bar{r}\ar@/_/[l]_{a,b} } &
\xymatrix{
        w\ar[d]_{a,b} & p \ar[l]_{b} \ar[d]^{b} \\
        r\ar@/_/[r]_{{a},{b}} & \bar{r}\ar@/_/[l]_{{a},{b}} }
\end{array}
$$
\caption{An operational account of the rain debate}
\label{fig:wrong}
\end{figure}

This is a neat and uncontroversial depiction of the actual utterance made, and it gives us a clear outcome. In particular, if we look at the AF (3) depicted in Figure \ref{fig:wrong} above, we can easily conclude that $p, r$ must be assigned the value true, so that $\bar r, w$ are both defeated and become false. Hence it follows that agent ${b}$ won the debate. But is this really the rational way of looking at things?  Intuitively speaking, it seems quite problematic. Why would a puddle be conclusive evidence, and stronger evidence than a weather report? For an objective audience, a weather report might carry some weight, and so might a puddle, but neither seem particularly conclusive.

We believe the problem is that the representation we gave in Figure \ref{fig:wrong} is too simplistic, since it fails to include information about the agents' view of each other's utterances. In a perfect world, this might not matter, since all debate might eventually be settled conclusively by brute empirical fact, such as observing actual rain as opposed to consulting weather reports and puddles. However, in such a world, deliberation would certainly not be very interesting, and it is not how it tends to play out in the real one. Rather, a debate involves crucially a search for common ground, and common ground depends crucially on how agents perceive the statements made by others, as they reflect on the totality of the debate. This is why we need to be explicit about subjective views, and always ask for a representation of how each individual agent interprets the semantic meaning of all those claims that are relevant to the scenario at hand.

In particular, what is missing in the naive model of the Bergen rain debate is some account of how agent $a$ views puddles, and what agent $b$ thinks of weather reports. Let us assume that their views on this are in fact the following.
\begin{figure}
$$\begin{array}{cc}
V_{a} \hspace{3em} & V_{b} \\
\xymatrix{ w \ar[d] \ar[r] & p \ar[d] \\ r \ar@/_/[r] & \bar r \ar@/_/[l] } \hspace{3em} &
\xymatrix{w \ar[d] & p \ar[d] \ar[l] \\ r \ar@/_/[r] & \bar r \ar@/_/[l] }
\end{array}$$
\caption{Two views on rain in Bergen}
\label{rainview}
\end{figure}
It seems clear that these views are consistent with the actual exchange of arguments described earlier, and that they might easily come to result in deliberative events that appear in this form. We notice, moreover, that according to the views depicted here, both $b$ and $a$ acknowledge that their respective arguments for and against rain are correct. But they also both think that their own argument is stronger, in that it attacks also the other agent's argument, but not vice versa. This might be the case, for instance, because the weather report gives $a$ reason to doubt that $b$ is telling the truth about the puddle, while seeing the puddle gives $b$ reason to doubt the relevance of the weather report. Importantly, it might not be rational or reasonable for $a$ and $b$ to disagree about how their arguments are related, but that they would do so is nevertheless consistent with the fact that reasoners often tend to display confirmation bias, putting more weight on evidence in support of previous beliefs.

Importantly, since we now use AFs to encode subjective views, we are in a position to reject the AFs that were used to encode the actual exchange of arguments in Figure \ref{fig:wrong}. The final AF here encodes an interpretation that includes the attack $(p,w)$. But agent $a$ disagrees with agent $b$ about the presence of this attack, so it is unwarranted to take it for granted that $(p,q)$ will come to influence the outcome of deliberation. Indeed, in order to even begin talking about the outcome we need first some aggregated view on the semantic meaning of the arguments involved, based on the agents' views rather than actual utterances. It seems to us that the function of deliberation is to generate such views, and in the next section we develop a logical framework based on this idea.

\section{Argumentative deliberation: A formalization using modal logic}\label{sec:ddl}

Given a basis which encodes agents' views of the arguments, we are interested in the possible ways in which agents can deliberate, and how deliberation can serve to create new, socially defined, interpretations of the arguments, interpretations that are aggregated from the views of the individual agents.  We want to study the \emph{effect} of deliberation on semantic meaning, which, in our opinion, is the crucial question that needs to be addressed in the search for new foundations for rational interaction.

Towards formalization, we will first define the set of all possible interpretations that may result from deliberation. These will be the states of our models, and we will represent them using AFs. At this stage there is only one requirement that we require such states to meet, namely that they are all based on the views of the agents. This is encoded in the following definition.
\begin{definition}\label{def:dstate}
Given an argumentative state $\views$, we say that $q \subseteq \Pi \times \Pi$ is a \emph{deliberative state} for $\views$ if
\begin{equation}\label{eq:ds}
\bigcap_{a \in \agents}V_a \subseteq q \subseteq \bigcup_{a \in \agents}V_a
\end{equation}
We collect all deliberative states for $\views$ in the set $\dstate \views$. 
\end{definition}

The requirement that states must be based on the agents' views means that we do not allow deliberation to result in states that deviate from interpretations that are held unanimously by the agents. If everyone agrees on the meaning of an argument, the argument has this meaning no matter how deliberation proceeds.\footnote{Interestingly, this does not mean that a possible unanimity regarding the semantic status of an argument is necessarily reflected in the view aggregated by deliberation, not even when all agents reason according to the same semantics. If the agents differ in their account of \emph{why} an argument should be accepted, in particular, deliberation might lead to its rejection. We will formalize a scenario like this in Section \ref{sec:examples}, Example \ref{ex:ad}.} Having defined the set of states, we can now define the notion of a \emph{deliberative model}.

\begin{definition}\label{def:dk}
Given a deliberative state $\views$, a deliberative model for $\views$ is a tuple $(Q,R)$ such that
\begin{itemize}
\item $Q \subseteq \dstate \views$ is a set of deliberative states for $\views$.
\item $R$ is a relation $R \subseteq Q \times Q$.
\end{itemize}
\end{definition}

The idea is that the relation $R$ encodes a process of deliberation based on the views in $\views$. If $(q_1,q_2) \in Q$ the intuition is that there is some event that can take place in the deliberative state $q_1$ so that the aggregated views of the arguments is updated, taking us to the deliberative state $q_2$. We abstract away from the deliberative events that can induce such a link, but this could be some agent presenting his point of view, or it could be some joint effort, say a vote, to reach a decision about some argument. In this paper, we will keep things simple and leave the exact content of events unspecified.

As an example of a deliberative model, consider the framework in Figure \ref{fig:del1}. Here, the argumentative state is problematic from the point of view of classical logic. In particular, we have $\views \models_\sem \neg \cdia_a x \land \neg \cdia_a \neg x$ under all $\sem$ from Figure \ref{fig:sem}, arising from the fact that in agent $a$'s view, the argument $x$ attacks itself and is not defeated. Hence it cannot be regarded as either true or false without leading to contradiction, and agent $a$ is prevented from reaching any classically sound conclusions about the status of either argument (since he also perceives $x$ to attack $y$). The agent $b$, on the other hand, has the view that $x$ and $y$ are in opposition to each other; If one of them is accepted the other must be rejected and vice versa. But he has no information which suggests choosing one over the other. In particular, we have $\views \models_\sem \cdia_b y \land \cdia_b \neg y$. Hence from his point of view, the semantic status of $x$ and $y$ remains unclear. Through deliberation, however, it is possible to arrive at a definite outcome which also resolves the inconsistency that $a$ believes to be present at $x$. 

One such scenario is depicted in Figure \ref{fig:sem}, where deliberation starts with the empty framework over $\{x,y\}$ and then proceeds by agent $a$ first putting forth his point of view, resulting in $q_1$, and then continuing with agent $b$ adding to this his own understanding, which results in the deliberative state $q_2 = V_a \cup V_b$. Here there is no problem, and the status of $x$ and $y$ has been definitely resolved, since $y$ must be accepted and then $x$ will be defeated, under all semantics from Figure \ref{fig:sem}.

\begin{figure}
$\begin{array}{llllll}
V_a: \xymatrix{& x \ar[r] \ar@(lu,ld) & y}, & V_b: \xymatrix{x \ar@/_/[r] & y \ar@/_/[l] } \\ \\
q_0: \xymatrix{ x & y }, & q_1: \xymatrix{& x \ar@(lu,ld)  \ar[r] & y }, & q_2: \xymatrix{&  x \ar@(lu,ld)  \ar@/_/[r] & y \ar@/_/[l]}
\end{array}$
\caption{A deliberative model $(Q,R)$ over $\views = (V_a,V_b)$ with $R = \{(q_0,q_1),(q_1,q_2)\}$}
\label{fig:del1}
\end{figure}

This is an example of a scenario where everything runs smoothly and there is no controversy. In particular, both agents uncritically accept adding each others' points of view to the aggregated deliberative state, resulting in the union of their views emerging as the final outcome of deliberation. Things might not be so simple, however, and it is the more complicated scenarios that can benefit the most from logical modeling. It could be, for instance, that agent $a$ has reservations about agent $b$'s interpretation of $y$ as an argument that also attacks $x$. If we are unsure about agent $a$'s stance in this regard, or, more generally, unsure about whether deliberation based on the views of agents $a$ and $b$ will eventually return a state where the $(y,x)$-attack is included, we can model this by introducing branching in the deliberative model. In particular, we could introduce a reflexive loop at $q_1$, to indicate the possibility that $b$'s perspective might come to be rejected. Then we have a branching deliberative model, and while it is still \emph{possible} to resolve the problems with original argumentative state, deliberation can then also fail to do so. 

To talk about deliberative models, allowing us to distinguish and identify situations such as these, we can use existing modal languages of varying expressive power. We will focus on making conceptual points, so we consider the following simple language $\lang_1$, which adds to $\lblack$ a modality for talking about the current deliberative state and the one-step possibilities in deliberative models.

$$
\phi := \cdia \alpha \ \mid \ \cdia_a \alpha \ \mid \ \neg \phi \ \mid \ \phi \land \phi \ \mid \adia \phi
$$
where $\alpha \in \lang$. We now use $\cdia$ as a modality to talk about the semantic status of arguments at a deliberative states, while the agent-indexed modalities still apply to the agents' views in the argumentative state. The definition of satisfaction for $\lang_1$ on deliberative models is then defined analogously to classical modal logic.

\begin{definition}\label{truth1}
Given an argumentation semantics $\sem$, an argumentative state $\views$ and a corresponding deliberative model $(Q,R)$, the truth of $\phi \in \lang_1$ on $(Q,R)$ at $q \in Q$ is defined inductively as follows (we omit the boolean cases).
\begin{itemize}
\item $\views, (Q,R),q\models_\sem \cdia \alpha$ if $\exists \clab \in \sem(q): \overline \clab(\alpha) = 1$
\item $\views,(Q,R),q\models_\sem \cdia_a \alpha$ if $\exists \clab \in \sem(V_a): \overline \clab(\alpha) = 1$
\item $\views,(Q,R),q \models_\sem \adia \phi$ if there is $q' \in Q$ s.t. $(q,q') \in R$ and $\views,(Q,R),q' \models_\sem \phi$
\end{itemize}
\end{definition}
 
We use the shorthand $\abox \phi := \neg \adia \neg \phi$ as usual. Let us consider the model from Figure \ref{fig:del1} as an example. Then it is easy to verify that $\views, (Q,R), q_0 \models_\sem \abox \abox \cbox \neg x$, expressing how two steps of deliberation will necessarily suffice to resolve $a$'s semantic problems with $x$ in this scenario, leading us to conclude $\neg x$ at the social level. However, if we add a reflexive edge $(q_1,q_1)$ to this model, to encode uncertainty about whether agent $b$'s view will survive deliberation, we obtain only the weaker $\views, (Q,R),q_0 \models_\sem \adia \adia \cdia \neg x$. It is still \emph{possible} that the problems at $x$ are resolved, but this is no longer necessarily so.

This toy example illustrates that with the machinery now in place we can formally describe how deliberation can sometimes turn individual views that classical logic and traditional notions of rationality deem problematic into deliberative states that are classically consistent. This is a mechanism that is stressed as being crucial in the argumentative theory of reason, and in Section \ref{sec:examples} we provide some more examples of how deliberative models provides a specification formalism capable of representing it, allowing us also to express and explore this mechanism using logic. However, we can also be more ambitious on behalf of the logical approach, seeking to move beyond mere modeling of concrete instances, towards formalization of theoretic concepts and principles. We provide a preliminary exploration of this line of inquiry in Section \ref{sub:for}, arguing that it shows great promise, but also that it suggests development of more subtle logical tools which can allow us to introduce more structure to our semantics, that we can then explore using the expressive power of more complex modal languages. 

\subsection{Using deliberative logic to model argumentative deliberation}\label{sec:examples}

In \cite{mercier}, one of the primary claims concerns confirmation bias, the mechanism by which reasoners disproportionately tend to favor reasons that support previous beliefs rather than challenge them. According to the authors, this bias is not necessarily an example of flawed reasoning since it has an argumentative function that can serve to enhance the positive effects of deliberation. This claim is supported by empirical evidence, and in this section we show how scenarios where cognitive bias plays such a constructive role can be represented by deliberative models and reasoned with using modal logic. Following this, we go on to consider some more examples which we believe illustrate that as an approach to modeling, the formal framework suggested in this paper appears to be both flexible and expressive. 

\begin{example}[Rain in Bergen revisited]
We return to the Bergen rain example, considered in depth in Section \ref{ex:run}. There we argued that instead of directly modeling the actual exchange of arguments, the deliberative events that took place, we should start from a representation of the agents' view of the arguments. These were given in Equation \ref{rainview}, and we argued that these views were consistent with the deliberative event under consideration. Now we can try again to model the deliberation that took place, more abstractly by seeing it as a traversal on a deliberative model. To do this, we will consider the possible effect that each utterance could have on the deliberative state, given a starting point where $r$ and $\overline r$ are in recognized mutual opposition to each other. We recall that the first event was that agent $a$ pointed out the weather report, and that the second event was agent $b$ pointing to the puddle. This leads us then naturally to consider the deliberative model $(Q,R)$, depicted in Figure \ref{rain}.
\begin{figure}
$\begin{array}{lll}
Q: & \hspace{0.3cm} & R: \\ 
\begin{array}{ccccccccc}
q_0 && q_1 && q_2 \\ 
\xymatrix{ w & p \\ r \ar@/_/[r] & \bar r \ar@/_/[l] } &&  \xymatrix{ w \ar[d] & p \\ r \ar@/_/[r] & \bar r \ar@/_/[l] } && \xymatrix{ w \ar[d] & p \ar[d] \\ r \ar@/_/[r] & \bar r \ar@/_/[l] } \\ \\
q_3 && q_4 && q_5 \\
\xymatrix{ w \ar[d] \ar[r] & p \ar[d] \\ r \ar@/_/[r] & \bar r \ar@/_/[l] } && \xymatrix{ w \ar[d] & p \ar[d] \ar[l] \\ r \ar@/_/[r] & \bar r \ar@/_/[l] } && \xymatrix{ w \ar@/_/[r] \ar[d]  & p \ar[d] \ar@/_/[l] \\ r \ar@/_/[r] & \bar r \ar@/_/[l] } \\ \\
\end{array} &\hspace{0.3cm} & 
\begin{array}{l} \\
\xymatrix{ & q_2 & q_4 \\
q_0 \ar[r] & q_1 \ar[ur] \ar[u] \ar[d] \ar[dr] \\ & q_3 & q_5 }
\end{array}
\end{array}$
\caption{The deliberative model $(Q,R)$ based on the argumentative state $\views$ defined in Figure \ref{rainview}}
\label{rain}
\end{figure}

What do we get from this model? Let us assume, for an intuitive presentation of the scenario, that the model describes a situation where deliberation is based on searching for agreement about what attacks \emph{not} to include. That is, unless the agents agree on something else we always end up in the deliberative state which is formed by taking the union of the agents' view. Then, if disagreement runs deep, the Bergen rain debate will end in state $q_5$. Here it is not hard to see that the question of whether or not it will rain remains unsettled -- both $\{w,\bar r\}$ and $\{r,p\}$ are admissible in the resulting AF. In this case, then, debating only served to establish the social fact that the question of whether it will rain in Bergen is still open in the social group $\{a, b\}$, both can make up his own mind and disagreement may persist. However, if the agents are willing to consider a consensus, then they can settle on either $r$ or $\bar r$, by moving to state $q_3$ or $q_4$ respectively. Moreover, they can also choose to conclude, in agreement, that the available evidence is \emph{insufficient} to draw any conclusion. This in particular, is the outcome resulting from the following deliberative state, which emerges from debate if the agents are prudent and reach agreement on including only those attacks that are present in both views. This means that they end in state $q_2$, and here both $w$ and $p$ are regarded as successful, meaning that \emph{both} $r$ and $\bar r$ becomes defeated and impossible to accept.\footnote{In this paper we only sketch a framework that permits us to logically examine spaces of possible outcomes, such as those identified by $(Q,R)$. We remark, however, that a natural next step is to try to investigate which one of these would actually result from cooperation, given some assumptions about the faculties of the agents involved, and depending on how arbitration takes place inside coalitions.} 

This consensus outcome is interesting since it seems plausible that in an actual debate, this is what one would get if the agents respect each others' input. Hence it might be the outcome we \emph{should} get. After all, it represent both a clear conclusion, and also a ``fair" one in light of the evidence, where neither agent looses on grounds that he believes to be unreasonable. More interesting still, it is a consequence of the model that this outcome is only achievable because the agents display confirmation bias with respect to their own arguments; logically, there is little doubt that the two arguments, pulling in opposite directions, attack each other. Yet from the fact that each agent underestimates his opponent's arguments beyond what is rationally warranted, a situation is created whereby deliberation may result in a non-trivial, reasonable interpretation that produces an unambiguous and fair outcome to end the disagreement regarding whether or not it will rain in Bergen today. There is, as usual in Bergen, no way of knowing.

\end{example}

\begin{example}[Two wrongs that make a right]\label{ex:wr}
Consider the following argumentative state:
$$
\begin{array}{llll}
V_a: & \xymatrix{ & u \ar@(ru,rd) \\ v \ar[ur] \ar[dr] \ar@(lu,ld) \\ & w \ar[uu] } & \hspace{1.5cm} V_b: & \xymatrix{ u \ar[dd] \\ & v \ar@(ru,rd) \ar[ul] \\ w \ar[ur] }
\end{array}
$$
Here the two agents both have an inconsistent view on the semantic elements, as the reader may easily verify. Moreover, the agents agree that $v$ attacks itself. But even so, deliberation can result in consistency being regained. In particular, the AF depicted below is a deliberative state for $(V_a,V_b)$ and it is easily seen to be classically consistent, under the evaluation $\{v \mapsto 0, w \mapsto 1, u \mapsto 0\}$.
$$
\xymatrix{ & u \ar@/_/[dd] \\ v \ar@(lu,ld) \ar[ur] \\ & w \ar[ul] \ar@/_/[uu] }
$$
In fact, we can say more about deliberation based on $(V_a,V_b)$. It is true, in particular, that we can evaluate the three arguments classically if and only if we end up in a deliberative state where $w$ is understood to attack both $v$ and $u$. How to ensure that our deliberation leads to this result is unclear, but at least we can recognize it as a possible way in which, for this argumentative state, two wrongs could in fact come to make it right.

\end{example}

\begin{example}[An agreement that disagrees with itself]\label{ex:ad}
While deliberation can sometimes take us from an inconsistent argumentative state to deliberative states that admit classical evaluation, the direction of deliberation can also go in the other direction. Consider, for instance, the following views
$$
\begin{array}{llll}
V_a: & \xymatrix{ u \ar[r] & v \ar@/_/[dl] \ar[d] \\ w \ar@/_/[ur] \ar[u] & p } &\hspace{1.5cm} V_b: & \xymatrix{u \ar@/_/[d] \ar[r] & v \ar[d] \ar[dl] \\ w \ar@/_/[u] & p }
\end{array}
$$

For $\views = (V_a,V_ b)$ we have $\views \models_\sem \cbox_a (\neg v \land p) \land \cbox_b (\neg v \land p)$. That is, the agents both have an interpretation of the world such that $v$ comes out false and $p$ comes out true under all labellings permitted by any of the semantics from Figure \ref{fig:sem}. Moreover, both views can be evaluated classically, as the reader may easily verify. But the agents disagree about something else, namely the status of $u$ and $w$. In particular, we have $\views \models_\sem \cbox_a (w \land \neg u) \land \cbox_b (\neg w \land u)$. This disagreement could spell disaster for deliberation taking place on the basis of $\views$. It is easy to see, in particular, that the following state $q$ could result through deliberation.
$$
\xymatrix{  u \ar[r] & v \ar[d] \ar[dl] \\ w \ar[u] & p }
$$
In this case there are \emph{no} classical evaluations available. That is, for any deliberative model containing $q$, we have $\views, (Q,R), q \models \neg \cdia x \land \neg \cdia \neg x$ for all $x \in \{o,v,w,p\}$. Hence deliberation, in this case, took unproblematic views and produced a problematic outcome. How to avoid this mechanism in general is a difficult problem that deserves further study, but we can already conclude that our model allows us to capture how deliberation can sometimes end up undermining agreements about the status of arguments, and can also lead to the introduction of \emph{new} inconsistencies. Indeed, this too is a mechanism that is intuitively clear. For instance, if one man happens to be right, for all the right reasons, while everyone else either disagrees with his conclusion or his reasons, then deliberation typically lead to suboptimal outcomes. This is important to remember, all the while the other, more positive potential in deliberation, is essentially just an expression of the same mechanism. The whole is more than the sum of its parts, and in order to explore and evaluate it we need new tools and principles that sees it as such.
\end{example}

\subsection{The search for formal characterizations of rational argumentative interaction}\label{sub:for}

Having set up a logical framework for describing argumentative deliberation, we are now also in a position to begin exploring formal representations of principles of rational argumentative interaction. We can think of such principles as restrictions on the class of deliberative models we consider permissible, allowing us to distinguish the ``good" scenarios from the ``bad" ones in a formally precise way. This can be done either semantically, in terms of direct mathematical definition of the appropriate classes of models, or axiomatically, by using formulas and schemata from some formal language to express key properties that we believe can serve to characterize good deliberation.

We have already seen how deliberative logic allows us to model scenarios where the outcome is classically sound even if all individual views are inconsistent. The requirement that deliberation should be organized in such a way that it \emph{always} functions in this way might then suggest itself as a good candidate for a normative notion of rationality imposed at the social level. It is a very strong notion, however, and it is potentially problematic also because it is not in fact wholly social. In particular, a requirement to the effect that the outcome of deliberation should always be classically consistent must by necessity also involve restriction on what individual views we permit agents to endorse. 

This is easy to see intuitively. The case of a system with a single agent who believes something absurd, for instance, or a system with many agents where all of them share an inconsistent interpretation of the world are obvious examples. The fact that deliberation alone cannot ensure a consistent outcome in such cases seems clear, and it is an insight that we can now express formally. First we must define an appropriate formalization of classical soundness at the level of deliberation, and how to do this is not obvious. We will discuss some subtleties regarding this later, but for now let us simply consider the following intuitive axiom schema for classically rational deliberation.
\begin{equation}\label{crat}
\cdia \neg (\phi \leftrightarrow \neg \phi)
\end{equation}
If we require it to be true on all models, in all points, we stipulate that for all claims we can express about the model, it should always be possible to reason about this claim in such a way that it is not considered to be equivalent to its own negation. The restriction is perhaps too weak, but at least it appears like a reasonable minimal requirement. We note, moreover, that the scheme is \emph{not} valid on the class of all deliberative models. Hence it captures a non-trivial principle, a genuine restriction on deliberation. However, we also notice that for some argumentative states $\views$, there are \emph{no} corresponding deliberative models such that Schema \ref{crat} holds. Therefore, if we impose it as an axiom of deliberation, we also restrict the class of permissible argumentative states, meaning that it does not provide a purely deliberative approach to rationality. This, in particular, is the formal expression of the intuition that constraints on deliberation alone is not enough to ensure classical consistency in all circumstances.

This recognition does not in itself imply that Schema \ref{crat} should be discarded. Rather, we think it is interesting to investigate further in it what ways this and similar schemata can still permit \emph{more} variety in the reasoning patterns of individuals than what is allowed under normative theories that presuppose classical reasoning at the individual level. Moreover, the discussion above suggests that we are now in a position to define the following two different kinds of social rationality principles, which can help us to provide more structure to future inquiries.

\begin{itemize}
\item Liberal principles: Rationality constraints that do not force us to restrict the set of argumentative states that we consider possible. 
\item Idealistic principles: Rationality constraints that require us to restrict the set of possible argumentative states.
\end{itemize} 

An example of a liberal principle could for instance be $\neg \abox (\neg p \land p)$, expressing that the relations used to form deliberative models must be serial, in just the same way as in classical modal logic. In the context of deliberation it would be the principle of that deliberation is open-ended, that there is always a deliberative next step (although at some point it might just be an endless repetition of previously visited states). A more subtle example, involving deliberative interactions, is the principle $\cdia \phi \to \adia \cdia \phi$ which expresses that if something is true in a deliberative state it should also be true in all following states, encoding commitment to previous outcomes. This is not a restriction on the kinds of relations that are allowed to exist between deliberative states, but rather a restriction on how deliberation is allowed to unfold from the argumentative state. Intuitively speaking, it restricts the kinds of deliberative events we allow. It is easy to see that it is liberal, however, since a single state without successors will always satisfy it. Note that if we add a loop to this state, it witnesses to the liberality of the principle which requires seriality plus commitment to previous outcomes; a debate that never ends and always increases the set of truths it produces.

For an example of an idealistic principle, it is enough to point out that Schema \ref{crat} is idealistic since it excludes certain argumentative states. In fact, we can provide a simple characterization of those argumentative states that are permitted. Let us say that an argumentative state $\views$ satisfies an axiom schema if there is some deliberative model based on this argumentative state for which the schema is true in all states. Moreover, let us say that an argumentative state $\views = (V_a)_{a \in \agents}$ is finite if $\Pi(V_a)$ is finite for all $a \in \agents$. Then we have the following result.

\begin{theorem}\label{thm:1}
For all semantics $\sem$ from Figure \ref{fig:sem}, a finite argumentative state $\views = (V_a)_{a \in \agents}$ satisfies Schema \ref{crat} if, and only if, there is some deliberative state $q$ for $\views$ which admits $\clab \in \sem(q)$ such that
$$
\forall x \in \bigcup_{a \in \agents}\Pi(V_a): \clab(x) \in \{1,0\}
$$
\end{theorem}

\begin{proof}
$\Leftarrow$) Immediate from Equation \ref{eq:lsem}. \\
$\Rightarrow$) We let $A = \bigcup_{a \in \agents}\Pi(V_a)$ and form the conjunction $\phi = \band_{x \in A}(x \lor \neg x)$. We notice that for all three-valued assignments $\clab$ we have $\bar \clab(\phi) \in \{1,\frac{1}{2}\}$ and that $\bar \clab(\phi) = \frac{1}{2}$ if, and only if, there is some $x \in A$ such that $\clab(x) = \frac{1}{2}$. Let $q$ be some deliberative state for $\views$ such that $\cdia \neg (\phi \leftrightarrow \neg \phi)$ is true on 
$q$ under $\sem$. Hence there is some $\clab \in \sem(q)$ such that $\clab(x) \in \{1,0\}$ for all $x \in A$, concluding the proof.
\end{proof}

This theorem is a simple illustration of the kinds of results we can obtain when we begin to formalize deliberative principles using logic. It also illustrates the subtleties involved, and how classical concepts might take on new and surprising forms. Notice, for instance, how the case of two wrongs that make a right, considered in Example \ref{ex:wr}, follows from the result. So according to Schema \ref{crat}, such a collection of views will be permitted, even if each view is not in itself classically consistent.

Rather than forbidding the views, we must instead exclude some deliberations that they can give rise to.
How to do this can be a tricky question, and the language of $\lang_1$ might be too weak to deliver principles for deliberation that allow us to get very far in this regard. Consider for instance a scenario where deliberation proceeds in a step-wise fashion, such that one argument is considered at a time starting from the deliberative state which contains no attacks. This protocol itself seems perfectly reasonable, but it now becomes highly unreasonable to require classical consistency at every state. It makes more sense to stipulate weaker notions such that it should \emph{eventually} hold, after deliberation has had a chance to work. In general, the ability to express that something holds eventually is an important addition to the expressive power of out logical language, which at once allows us to consider a whole range of interesting questions.

In the following we merely sketch some of these, intended to serve as an illustration of the great potential for using stronger modal languages to reason about deliberative models. In particular, we define the modality $\adia^\ast \phi$, intuitively to be read as saying ``after finitely many steps, $\phi$ becomes true". Formally, we let $\adia^n \phi$ denote $\overbrace{\adia \adia \ldots \adia}^{n} \phi$ and define satisfaction for $\adia^\ast \phi$ inductively as follows
\begin{equation}\label{sem:star}
\views,(Q,R),q \models_\sem \adia^\ast\phi \text{ if there is } n \in \mathbf N: \views,(q,R),q\models_\sem \adia^n \phi
\end{equation}

We also define $\abox^\ast \phi := \neg \adia^\ast \neg \phi$. This then expresses ``always $\phi$". With these constructs in hand we can express many subtly different properties of deliberation, some of which might be seen as candidates for rationality principles.

Let us assume that $\phi$ expresses some principle which we take to define ``good" states in a normative theory. For instance, $\phi$ could be an instance of Schema \ref{crat}. However, even if we believe that $\phi$ captures some essential normative requirement on the outcome of deliberation, it is not at all clear that we should require \emph{all} states in a deliberative model to be good states. Indeed, it can often seem more natural to think of deliberation as a process that complies with a norm of rationality just in case it is capable of taking us from bad to good states. In this case, we are cheating it we try to implement our normative theory by simply forbidding the bad states. Instead, we might want to restate our principle $\phi$ in one of the following ways, as a requirement on what it should be possible to achieve through deliberation, from the current state, rather than a requirement on what the current state itself should be like.

\begin{itemize}
\item $\abox \phi$; all deliberative events take us to a state where $\phi$ is true.
\item $\adia \phi$; there is at least one event taking us to a state where $\phi$ is true.
\item $\adia^\ast \phi$; there is a chain of events such that $\phi$ eventually becomes true.
\item $\adia^\ast \abox \phi$; there is a chain of events taking us to a state where every event will make $\phi$ true.
\item $\adia^\ast \abox^\ast \phi$; there is a chain of events taking us to a state where no further chain of events can make $\phi$ false.
\item $\abox^\ast \adia^\ast \phi$; for every chain of events, there is a way to continue this chain so that $\phi$ eventually becomes true.
\item $\abox^\ast \adia^\ast \abox^\ast \phi$; for every chain of events, there is a continuation so that $\phi$ eventually becomes true, and remains true forever.
\end{itemize}

These are some examples of notions that can be formulated in temporal logic. What makes them interesting to us is that in the present setting they express properties of deliberation. In the context of temporal logic they are quite well understood, but when $\phi$ also involves occurrences of modalities such as $\cdia$, expressing argumentative properties of the states, new questions arise, both technical and philosophical. One of the most interesting concerns the question of expressive power, and in future work we hope to consider various temporal and fixed-point languages and to investigate what they allow us to say about deliberative models. Moreover, we think it will be very fruitful to combine this technical work with addressing the philosophical challenge of developing a better foundational understanding of the nature of argumentative deliberation. To formulate and study normative principles of deliberative rationality axiomatically is a particularly interesting aspect of this work, and we believe the preliminary investigation carried out here shows its promise.

\section{Conclusion}\label{sec:conc}

We have presented a formal approach to the argumentative theory of reason and argued that it should be studied further, also using logical tools. To us, the argumentative theory is particularly interesting because it suggests an alternative approach to rationality, one which sees it as irreducibly social and emergent from deliberation. As such we think it can contribute important insight to the search for new theoretical foundations for rationality, foundations that could prove relevant to a range of different fields, including artificial intelligence, which increasingly focuses its attention on social processes and the study of rational and intelligent interaction.

For future work we hope that the simple formal framework developed in this paper can provide a point of departure for the development of more sophisticated technical tools. In the first instance, these should be developed so that they allow clear and informative mathematical modeling of important aspects of argumentative deliberation. But we also think that such formalisms can facilitate a highly interesting axiomatic approach to deliberative rationality,  allowing formal logical investigation of foundational issues, assisted by the use of temporal and fixed-point modal languages interpreted on argumentative structures. On the technical side, we think the study of the expressive power of various languages that allow interactions between computational and argumentative modalities is highly interesting and should be considered further. 

The study of deliberative models is the study of how representations of meaning and communicative events together give rise to branching structures of deliberation, where what is truly sound and rational might be hidden somewhere in a combinatorial object, a recurring pattern, or else not emerge at all except in the limit. Wherever it is hiding, and whatever its true nature might be, we believe the pursuit itself has great value, and that it should continue. In the end, perhaps the search for truth is itself a deliberative imperative, the normative power of which is derived not from the fact that it may some day settle, but from the fact that it must always be carried on.

\bibliography{pp}
\bibliographystyle{apalike}
\end{document}

