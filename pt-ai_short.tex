\documentclass[greybox]{svmult}

\usepackage{mathptmx}       % selects Times Roman as basic font
\usepackage{helvet}         % selects Helvetica as sans-serif font
\usepackage{courier}        % selects Courier as typewriter font
\usepackage{type1cm}        % activate if the above 3 fonts are


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cancel}
\usepackage{allrunes}
\usepackage{graphics}
\usepackage{verbatim}
\usepackage{color}
\usepackage{tikz}
\usepackage[curve]{xypic}
\usetikzlibrary{arrows}
\usepackage{algorithm2e}

\input{acros.tex}

\newcommand{\noo}[1]{}
\renewcommand{\bar}[1]{\overline{#1}}
\newcommand{\outa}[2]{#1^+(#2)}
\newcommand{\ina}[2]{#1^-(#2)}
\newcommand{\neu}[2]{#1^n(#2)}
\newcommand{\proto}{\mathbf C}
\newcommand{\clab}{\mathsf c}
\newcommand{\iterate}[1]{{\cal I}(#1)}
\newcommand{\cl}{cl}
\newcommand{\dstate}[1]{\mathsf D({#1}) }
\newcommand{\witness}[1]{{\mathsf w^{#1}}}
\newcommand{\buffer}[2]{{\mathsf b}_{#1}(#2)}
\newcommand{\sdepth}[2]{d_{#1}(#2)}
\newcommand{\shrink}[2]{\rho_{#1}(#2)}
\newcommand{\comp}[2]{C(#1,#2)}
\newcommand{\test}[1]{{#1}?}
\newcommand{\dlangm}{{\mathcal L}_{\acro{DDL}}}
\newcommand{\pis}[1]{{\mathbf e}_{#1}}
\newcommand{\carriers}[1]{Q_{#1}}
\newcommand{\kmod}[2]{{\cal K}_{(#1,#2)}}
\newcommand{\rels}[1]{{\mathsf R_{#1}}}
\newcommand{\update}[3]{{\mathcal U}_{#1}(#2,#3)}
\newcommand{\cons}[1]{{\textara{\ea}}(#1)}
\newcommand{\af}{{\mathsf F}}
\newcommand{\afn}{S}
\newcommand{\afe}{E}
\newcommand{\seq}[1]{\overrightarrow{#1}}
\newcommand{\basis}{basis }
\newcommand{\state}{state }
\newcommand{\views}{\mathcal B}
\newcommand{\viewsv}{\left(V_a\right)_{(a \in \agents)}}
\newcommand{\carrier}{Q_\views}
\newcommand{\sem}{\varepsilon}
\newcommand{\depth}[1]{|{#1}|^\adia}
\newcommand{\bisim}{\underline{\leftrightarrow}}

\begin{document}
\title*{Arguably argumentative: A formal approach to the argumentative theory of reason}
\author{Sjur K. Dyrkolbotn and Truls Pedersen}
\institute{Sjur K. Dyrkolbotn \at Durham Law School, Durham University, \email{s.k.dyrkolbotn@durham.ac.uk}
\and Truls Pedersen \at Department of Information Science and Media Studies, University of Bergen}

\maketitle

\abstract{
We propose a formal approach to the argumentative theory of reason, combining argumentation theory and modal logic in a novel way. We show that the resulting framework can be used to model important mechanisms identified by the theory, including how confirmation bias and other problematic modes of reasoning may in fact serve an important argumentative purpose that can give rise to classically sound conclusions through the process of social deliberation. We go on to suggest that the argumentative theory is based on an understanding of intelligent reasoning and rationality that sees these notions as irreducibly social, and that the argumentative theory itself provides a possible starting point in the search for new theoretical foundations based on this understanding. Moreover, we suggest that formal logic can be used also as a means to investigate foundational issues, and we sketch the development of an axiomatic approach to the study of rational deliberation.}

\section{Introduction}\label{sec:intro}

The idea that social interaction and rationality are mutual dependent is becoming increasingly important in many different fields of research, including economy, law, biology and artificial intelligence \cite{blume,terrell,waal,benthem,ossowski}. In all these research areas, there is a trend towards viewing rationality as fundamentally embedded in a social context, and this context is seen as important not only because people are social and tend to interact, but also because \emph{who} they are, \emph{what} they want, and \emph{why} they want it tends to depend on how they engage with each other and their environment.\footnote{This perspective has long been influential in political philosophy, sociology and social psychology, particularly in research traditions going back to the work of George Herbert Mead and the Chicago school \cite{mead}. But to many other fields, particularly those based on formal methods or rational actor models, it represents an important recent trend, a move away from methodological individualism towards more holistic approaches. Important work has also been devoted to attempting to unite the two paradigms, such as \cite{list} which presents a deliberative approach to social choice theory.}

To accommodate this point of view across different domains, we need better theoretical foundations, allowing us to investigate the relationship between reasoning and interaction, based on the starting point that they are co-dependent and co-evolving. In this paper, we argue that this challenge can be addressed using formal logic, drawing on tools and techniques developed in the context of multi-agent systems.\footnote{The connection between various branches of social science and formal logic and computer science has received much attention in recent years, and it has led to a surge of interest in interdisciplinary research \cite{parikh,benthem2,verbrugge}. However, while much recent work in applied logic has been devoted to modeling agency and interaction, the usual starting point is still that agents reason in adherence to some common standards of correctness, specified by formal logic. Also, it is typically assumed that rational interaction emerges from the fact that agents are individually rational in some appropriate sense, for instance because they seek to maximize given utility functions. In this paper we will argue that in order to provide adequate formal foundations for rational interaction we must depart from such reductionist assumptions.} In particular, we propose a formal approach to the argumentative theory of reason, introduced in \cite{mercier}. The key idea is that reasoning evolved to facilitate efficient argumentation, not necessarily to help us arrive at logically correct forms of inference, or in making reasonable decisions.

This can have important implications for logical modeling of rational interaction, and we sketch the development of a general logical framework that enables us to capture the idea formally. Our overreaching aim is to argue that a formal approach to the argumentative theory of reason can provide interesting new insights, particularly regarding the socially emergent nature of rationality. 

The structure of the paper is as follows. In Section \ref{sec:arg} we present the argumentative theory, focusing on its implications for our understanding of agency. We argue that the notion of argumentation that is at work challenges existing traditions in argumentation theory, particularly formal theories, and we go on to present an alternative formalization which looks at argumentative structures in a new light, as the basis upon which agents' subjective interpretations of the world are formed. In short, we introduce the \emph{argumentative agent}, and we argue that he should be studied further.

In Section \ref{sec:ddl} we go on to present a logical formalism for studying what we call \emph{argumentative deliberation}, interaction between argumentative agents that may serve to generate novel interpretations of the world. Since our purpose in this paper is to focus on main ideas rather than technical details, we present a simple logical framework, containing some basic constructions which can be developed further using existing tools from modal logic. We show through examples that this is sufficient to allow us to capture essential aspects of the mechanisms addressed in \cite{mercier}, and we also sketch the development of an axiomatic approach to deliberative rationality based on the argumentative theory. In Section \ref{sec:conc} we offer a conclusion with suggestions for future work.

\section{Argumentative agents: A semantics for individual reasoning based on argumentation}\label{sec:arg}

The argumentative theory of reason is formulated on the basis of experimental evidence which appears to suggest that human reasoning evolved to facilitate efficient argumentation. From the point of view of an individual, it seems that the most significant purpose of reason is not in helping him to arrive at logically correct forms of inference, but to maximize his chance of winning arguments. In terms of rational choice terminology, one might express this by saying that agents' utility functions are heavily influenced by their desire to do well when they argue with others, often to the detriment of sound classical reasoning.

This insight is interesting in itself, but it is also connected to a second insight stemming from the argumentative theory, regarding the conceptual foundation for our understanding of intelligence and rationality. 
In particular, the argumentative theory suggests looking for rationality principles that do not target individual reasoners at all, but rather the deliberative processes that they partake in. It is striking how deliberation can often lead to classically sound outcomes even if each individual reasoner is argumentative or even unreasonable, and we follow \cite{mercier} in thinking that this mechanism is crucial. It can serve to explain why argumentative reasoning has proved so successful for the human race, even if it regularly leads to unsound, or even absurd, results, when people reason in isolation. 

To develop a formal account of argumentative deliberation we will start from a formal representation of the reasoners themselves. In this regard, it is important to note that the argumentative theory involves a notion of argumentation which is conceptually distinct from that found in traditional argumentation theory, as it has been developed in modern times following the influential work of \cite{toulmin} (first edition from 1958). In this research tradition, the focus tends to be directed towards recognizing and categorizing fallacies, as well as the design of argumentation schemes that are meant to facilitate sound and rational reasoning, particularly regarding what arguments we should accept in a given scenario. The argumentative theory, on the other hand, asks us to look at argumentation more descriptively, and to take seriously the fact that reasoning appears to have evolved as a mechanism to facilitate \emph{efficient} argumentation, that might not conform to any general standards of correctness.

In the following we will formally represent argumentative agents using argumentation frameworks, first introduced in \cite{dung}. These are simple mathematical objects, essentially directed graphs, which facilitate the investigation of a whole range of semantics \cite{baroni}.\footnote{The theory of argumentation frameworks has been influential in the context of artificial intelligence \cite{rahwan}. It is capable of capturing many different semantic notions, including semantics for multi-valued and non-monotonic logics, logic programs and games \cite{dung,dyrkolbotn}. The work of \cite{brewka}, on the other hand, shows how argumentation frameworks can be used to provide a faithful (and computationally efficient) representation also of semantics that are formulated with respect to the more fine-grained formalism of abstract dialectical frameworks \cite{brewka1}. It is also important to note that much recent work focuses on providing logical foundations for the theory, work we can draw on when we develop multi-agent extensions  \cite{grossi,grossi1,arieli,caminada}.} We think they are well suited as a technical starting point towards logics for argumentative deliberation, but we propose to make use of them in a novel way, to represent the agents' subjective interpretations of semantic meaning.\footnote{In terms of each individual agent, using terminology from cognitive science, this places the argumentation framework at the informational level of cognitive processing, where previous work have already shown that logical tools can have a particularly crucial role to play, also serving to shed new light on established truths arrived at through empirical work, see e.g., \cite{stenning}.} This makes good sense with respect to the argumentative theory; Since agents reason to win arguments, it is natural to assume that they tend to represent semantic information in argumentative terms.

In addition, using argumentation frameworks provides us with a flexible framework for modeling many different ways in which individual agents may choose to reason, using one among the many argumentation semantics that have been developed. Hence we make no commitment to a given set of reasoning rules -- the argumentative agent is defined by the fact that he maintains an argumentative interpretation of the world, not by the fact that he reasons about it in a given way.

%While much work on multi-agent argumentation has already been carried out in a formal and semi-formal context, we note that this work is mostly based on a traditional view of argumentation theory.\footnote{For instance, we think this is implicit in recent formal work such as that of \cite{pigozzi,pigozzi1} and even more so in the survey of the field given in \cite{rahwan}.} In our opinion, however, this view is inappropriate when attempting to formalize the argumentative theory. The problem is that the representation of the argumentation scenario is fixed and not open to dispute and dynamic change, except with respect to the question of how it should be evaluated. But to model the argumentative theory, we need to depart from this starting point, since it is crucial that the basic representation of the surrounding semantic reality is itself a subjective construction, distinctly produced in each individual agent. This is why we use argumentation frameworks as models of the agents' internal view of the relevant arguments and how they are related. We return to the dynamics of deliberation in Section \ref{sec:ddl}, but first we offer a technical presentation of argumentation frameworks, and how they can be used to define a logic for talking about the views of argumentative agents.

\subsection{Argumentation frameworks, agents and semantic views}\label{subsec:arg}

In this section we formally define a logic for argumentative agents, starting with an introduction to argumentation frameworks. Given a set of semantic atoms $\Pi$, which we will tend to think of as names of arguments, an argumentation framework (AF) over $\Pi$ is a relation $E \subseteq \Pi \times \Pi$. Intuitively, an element $(x,y) \in E$ encodes the fact that $x$ attacks $y$. We can depict $E$ as a directed graph, giving a nice visualization of how the atoms in $\Pi$ are related as arguments, see Figure \ref{fig:1} for an example. 
We introduce the notation $\outa E x = \{y \in \Pi \mid (x,y) \in E\}, \ina E x = \{y \in \Pi \mid (y,x) \in E\}$, extended to sets $A \subseteq \Pi$ such that, e.g., $E^+(A) = \{y \in \Pi \mid \exists x \in A: y \in \outa E x\}$. We use $\Pi(E) = \{x \in \Pi \mid \forall y\in \Pi: \{(x,y),(y,x)\} \cap E = \emptyset\}$, denoting atoms from $\Pi$ that do not appear in any attack from $E$.

\begin{figure}
$$
\xymatrix{p \ar@(lu,ld) \ar@/_/[r] & q \ar@/_/[l] \ar@/_/[r] & q' \ar@/_/[l] \ar@/_/[r] & p' \ar@(ru,rd) \ar@/_/[l] }
$$
\caption{An AF $E$ such that $\Pi(E) = \{p,q,q',p'\}$}
\label{fig:1}
\end{figure}

Given an AF $E$, the purpose of an argumentation semantics is to identify a 
collection of acceptable sets of arguments, typically called \emph{extensions}. For instance, if $E = \{(p,q),(r,p)\}$, then the semantics might prescribe $\{r,q\}$ as a set that can be accepted, since $r$ defends $q$ against the argument made by $p$ and $r$ is not in turn attacked. 

Given an AF $E$ and an extension $A \subseteq \Pi$, the associated three-valued assignment is $\clab_A:\Pi \to \three$ given by
$$
\clab_A(x) = \begin{cases} 1 \text{ if } x \in A \\ 0 \text{ if } x \in \outa E A \\ \frac{1}{2} \text{ otherwise } \end{cases}
$$
This representation corresponds to an intuitive reading where atoms in $A$ are regarded as true/successful, atoms attacked by one of these are regarded as false/failed, and all others are undecided. The three-valued representation facilitates elegant definitions of various argumentation semantics, specifying sets of assignments rather than extensions. In this way, it also becomes natural to reason about AFs using three-valued logic, an idea that has been explored in some recent work \cite{dyrkolbotn,arieli,dyrkolbotn1}. This will be exploited in the coming sections, as we will rely on three-valued {\L}ukasiewicz logic when we evaluate complex formulas over given AFs.

All semantics for argumentation of which we are aware are based on the notion of \emph{conflict-freeness}, encoding that it is not allowed to assign $1$ to two atoms in conflict. Formally, we say that $\clab$ is conflict-free for $E$ if the following holds, for all $x \in \Pi$.
\begin{equation}\label{eq:cam}
\begin{array}{l}
\clab(x) = 0 \iff \exists y \in E^-(x): \clab(y) = 1
\end{array}
\end{equation}
We use $\proto(E)$ to denote the set of conflict-free assignments for $E$, and we define $\clab^1 = \{x \in \Pi \mid \clab(x) = 1\}, \clab^0 = \{x \in \Pi \mid \clab(x) = 0\}$ and $\clab^{\frac{1}{2}} = \{x \in \Pi \mid \clab(x) = \frac{1}{2}\}$. In Figure \ref{fig:sem} we provide definitions of the most commonly known semantics based on argumentation frameworks, all based on restricting $\proto(E)$. 

The logics we introduce are parameterized by the choice of an argumentation semantics, and which we use is not crucial for our analysis in this paper. We note, however, that the \emph{admissible} semantics encode what seems to be minimal criteria for intuitive acceptability of arguments. In addition to requiring that $\clab^1$ must be free from internal conflict, it also demands that it must be able to defend itself against all attacks. The semantics from Figure \ref{fig:sem} are all based on this idea, but the others involve various additional requirements, all meant to reduce the number of undecided arguments. In the following we use $\sem$ refer to some generic semantics from this list.

\begin{figure}
$\begin{array}{ll}
\text{\footnotesize{Admissible: }} & a(E) = \{\clab \in \proto(E) \mid  E^-(\clab^1) \subseteq \clab^0\} \\
\text{\footnotesize{Complete:}} & c(E) =  \{\clab \in \proto(E) \mid \clab^1 = \{x \in \Pi \mid E^-(x) \subseteq \clab^0\}\} \\
%\forall x \in \Pi: \\ & \clab(x) = 1 \iff \forall y \in E^-(x): \clab(y) = 0\} \\
%\text{\footnotesize{Grounded:}} & g(E) = \{\bigcap c(E)\} \\
\text{\small{Preferred:}} \ \ & p(E) = \{\clab_1 \in a(E) \mid \forall \clab_2 \in a(E): \clab^1_1 \not \subset \clab^1_2\} \\
\text{\small{Semi-stable:}} \ \ & ss(E) = \{\clab_1 \in a(E) \mid \forall \clab_2 \in a(E): \clab^{\frac{1}{2}}_1 \not \supset \clab^{\frac{1}{2}}_2\} \\
\text{\small{Stable:}} \ \ & s(E) = \{\clab \in a(E) \mid \clab^{\frac{1}{2}} = \emptyset\} 
\end{array}$
\caption{Various semantics, defined for any $E \subseteq \Pi \times \Pi$}
\label{fig:sem}
\end{figure}

Towards a logic for reasoning about AFs we will start from the simple propositional language $\lang$, defined by the grammar in Equation \ref{grammar}.

\begin{equation}\label{grammar}
\alpha := \ p \ \mid \ \neg \alpha \ \mid \ \alpha \to \alpha 
\end{equation}
where $p \in \Pi$. The three-valued assignments give rise to evaluations of arbitrary formulas from $\lang$, defined as in {\L}ukasiewicz logic, see Equation \ref{eq:lsem} below.
\begin{equation}\label{eq:lsem}
\begin{array}{l}
\overline \clab(p) = \clab(p) \text{ for } p \in \Pi \\
\overline \clab(\neg \alpha) = 1 - \overline \clab(\alpha) \\
\overline \clab(\alpha \to \beta) = min\{1,1-(\overline \clab(\alpha) - \overline \clab(\beta))\}
\end{array}
\end{equation}

This evaluation behaves like classical logic on the semantic values $\{0,1\}$. For an intuition of how the third value is dealt with, notice that the definition ensures that $\clab(p) = \frac{1}{2}$ if, and only if, we have $\overline \clab(p \leftrightarrow \neg p) = 1$. So we can express that a given atom is undecided, and this is what makes it so natural to use {\L}ukasiewicz logic, as opposed to some other three-valued formalism. 

We are now prepared to offer our definition of an argumentative agent. To this end, let $\agents$ be a set of agent names. Then a \emph{view} for agent $a \in \agents$ is an AF $V_a \subseteq \Pi \times \Pi$. It encodes his interpretation of the meaning of the arguments under consideration. We also define an \emph{argumentative state} as a tuple $(V_a)_{a \in \agents}$, associating a view with each agent. In this paper, we will assume for simplicity that the argumentative state remains the same throughout the course of deliberation, so that the views of the agents are not themselves subject to revision as the debate unfolds. This assumption should be relaxed in future work, by nested application of the ideas we develop in the next sections.

Given an agent $a \in \agents$ with a view $V_a$, we can use a modality $\cdia_a$ to perform meta-reasoning about the acceptance status of arguments on AFs, under some arbitrary semantics $\sem$. This idea lead us to the following multi-agent language $\lblack$.
$$
\phi := \cdia_a \alpha \ \mid \ \neg \phi \ \mid \ \phi \land \phi $$ where $\alpha \in \lang$ and $a \in \agents$. 

Given an argumentative state $\views = (V_a)_{a\in \agents}$, we can now define truth for formulas from $\lblack$ inductively as follows, for all formulas $\phi$.
\begin{equation}\label{eq:asem}
\begin{array}{l}
\views \models_\sem \cdia_a \alpha \text{ if there is } \clab \in \sem(V_a) \text{ s.t. } \overline \clab(\alpha) = 1 \\
\views \models_\sem \neg \phi \text{ if not } \views \models_\sem \phi \\
\views \models_\sem \phi \land \psi \text{ if } \views \models_\sem \phi \text{ and } \views \models_\sem \psi 
\end{array}
\end{equation}

Assume we have a single agent $a$ and that his view $V_a$ is given by the AF in Figure \ref{fig:1}. Then it is easily verified that the following claims holds for all semantics $\sem$.
\begin{itemize}
\item $V_a \models_\sem \cdia_a q$
\item $V_a \models_\sem \neg \cdia_a p \land \cdia_a \neg p$
\item $V_a \models_\sem \cdia_a (\neg q \land q')$
\item $V_a \models_\sem \cbox_a (\neg q \to (p \leftrightarrow \neg p))$
\end{itemize}

We can now begin to explore formally what we mean by classically sound reasoning. Since evaluation of complex formulas agrees with classical logic on the boolean values, it is tempting to say that an agent reasons classically if the argumentation semantics he uses always returns two-valued assignments. However, the core requirements underlying the semantics from Figure \ref{fig:sem} also seem to capture some crucial aspects of what we mean by classical soundness. Most importantly, they all disallow assignments where two arguments in conflict are assigned $1$, corresponding intuitively to the law of non-contradiction. Hence it appears that the stable semantics, which always require assignments that are both admissible and boolean valued, is the best candidate we have for a formalization of classical reasoning about AFs.\footnote{There is strong formal evidence supporting the claim that classical reasoning about AFs is captured by the stable semantics, in particular the result that AFs under the stable semantics provide a normal form for theories of propositional logic \cite{bezem}.}

Interestingly, if the stable semantics captures classical reasoning, we can easily conclude that there are interpretations of semantic reality that makes such reasoning impossible. Consider, for instance, the argument that agent $a$ considers as attacking itself. It is easy to see that it does not admit any stable assignment, and that it must by necessity obtain the value $\frac{1}{2}$ under all other semantics from Figure \ref{fig:sem}. In terms of logic, the self-attacking argument satisfies the formula $\cbox_a (x \leftrightarrow \neg x)$, expressing that $x$ is necessarily equivalent to its own negation.

Having defined classical reasoning by the stable semantics, we can also attempt to characterize the necessary failure of such reasoning in terms of graph structures for which no stable assignments exists. In fact, the combinatorial problem of when an AF admits a stable set has been analyzed in graph theory since the 1950s, by researchers using a different terminology, in the field of \emph{kernel theory}. The core result from this field, due to \cite{richardson}, immediately implies that any finite AF which has no directed odd cycle of attack admits a non-empty set of stable assignments.\footnote{This result was also rediscovered in \cite{dung}, but kernel theory offers many additional results and techniques, see for instance \cite{sanches}. These result can be understood as providing conditions which ensure the possibility of imposing classical standards of reasoning on agents' interpretations of the world, establishing an interesting link between the formalism in this paper and an established subfield of graph theory.}

\subsection{Extended example: Rain in Bergen}\label{ex:run}

We now analyze a simple example to motivate the need for introducing subjective views, suggesting also some shortcomings of a traditional approach to argumentation in the context of multi-agent deliberation. For a concrete scenario we consider two agents $a,b$ who argue about whether it will rain in Bergen today. Formally, $r$ represents the claim that it will rain and $\overline r$ represents the claim that it will not. We first assume that neither of the agents argue any further in favor of their positions. Then their basic claims are the only semantic entities present, and the AF shown on the left below depicts their relationship. So far, the model appears to be an uncontroversial objective representation of the state of affairs. There is not yet any discernible need for introducing subjective views. 
$$\begin{array}{ccc}
\xymatrix{ r \ar@/_/[r] & \overline r \ar@/_/[l] } & \hspace{3em} &
\xymatrix{ r \ar@/_/[r]_{{ a},{ b}} & \bar r \ar@/_/[l]_{{ a},{ b}} }
\end{array}$$

On the right above, we show a simple agent-indexed AF which illustrates a naive attempt at introducing agency to the initial AF. We label the two attacks, from $r$ to $\overline r$ and from $\overline r$ to $r$, by both $a$ and $b$ to encode that they are \emph{common} to the agents. That is, both agents acknowledge that these attacks are present -- the agents agree that they disagree.

The rational outcome, what claim we \emph{should} accept, remains unclear. Both $r$ and $\bar r$ seem acceptable, since no further arguments have been made. Indeed, our formal logic agrees -- either $r$ or $\bar r$ (but not both) can be taken as true, for all semantics from Figure \ref{fig:sem}.

Assume that $a$ and $b$ begin to argue in favor of their claims. For simplicity we consider only two steps of debate: first $a$ introduces the argument that the weather report says that it will not rain, and then $b$ counters this by announcing that he has seen a puddle on the pavement, suggesting that it will be a rainy day. Let us call the arguments provided by the weather report and the puddle $w$ and $p$ respectively. Then, noting that $w$ is an argument used by $a$ against $r$ and that $p$ is an argument used by $b$ as a retort against $w$ and also, let us assume, directly against $\bar r$, a naive representation in the traditional spirit would be to view the debate as progressing from $(1)$ to $(3)$, as depicted in Figure \ref{fig:wrong}.
\begin{figure}
$$
\begin{array}{ccc}
(1): & (2): & (3): \\
\xymatrix{ \\ r\ar@/_/[r]_{a,b} & \bar{r}\ar@/_/[l]_{a,b} } &
\xymatrix{
        w\ar[d]_{a} & \\
        r\ar@/_/[r]_{a,b} & \bar{r}\ar@/_/[l]_{a,b} } &
\xymatrix{
        w\ar[d]_{a,b} & p \ar[l]_{b} \ar[d]^{b} \\
        r\ar@/_/[r]_{{a},{b}} & \bar{r}\ar@/_/[l]_{{a},{b}} }
\end{array}
$$
\caption{An operational account of the rain debate}
\label{fig:wrong}
\end{figure}

This is an uncontroversial depiction of the actual utterance made, but it results in a dubious model. In particular, if we look at the AF (3) depicted in Figure \ref{fig:wrong} above, we can easily conclude that $p, r$ must be assigned the value true, so that $\bar r, w$ are both defeated and become false. Hence it follows that agent ${b}$ won the debate. But is this really the rational way of looking at things?  Intuitively speaking, it seems problematic. Why would a puddle be conclusive evidence, and stronger evidence than a weather report? For an objective audience, a weather report might carry some weight, and so might a puddle, but neither seem particularly conclusive.

We believe the problem is that the representation we gave in Figure \ref{fig:wrong} is too simplistic, since it fails to include information about the agents' view of each other's utterances.\footnote{In a perfect world, this might not matter, since all debate might eventually be settled conclusively by brute empirical fact, such as observing actual rain as opposed to consulting weather reports and puddles. However, in such a world, deliberation would certainly not be very interesting, and it is not how it tends to play out in the real one. Rather, a debate involves crucially a search for common ground, and common ground depends crucially on how agents perceive the statements made by others, as they reflect on the totality of the debate. This is why we need to be explicit about subjective views, and always ask for a representation of how each individual agent interprets the semantic meaning of all those claims that are relevant to the scenario at hand.}
In particular, what is missing is some account of how agent $a$ views puddles, and what agent $b$ thinks of weather reports. Let us assume that their views are in fact those depicted in Figure \ref{rainview}.
\begin{figure}
$$\begin{array}{cc}
V_{a} \hspace{3em} & V_{b} \\
\xymatrix{ w \ar[d] \ar[r] & p \ar[d] \\ r \ar@/_/[r] & \bar r \ar@/_/[l] } \hspace{3em} &
\xymatrix{w \ar[d] & p \ar[d] \ar[l] \\ r \ar@/_/[r] & \bar r \ar@/_/[l] }
\end{array}$$
\caption{Two views on rain in Bergen}
\label{rainview}
\end{figure}
These views are clearly consistent with the actual exchange that took place. We also notice that both agents acknowledge that their respective arguments for and against rain are correct. However, they also both think that their own argument is stronger, in that it attacks also the other agent's argument, but not vice versa. This might be the case, for instance, because the weather report gives $a$ reason to doubt that $b$ is telling the truth about the puddle, while seeing the puddle gives $b$ reason to doubt the relevance of the weather report. Importantly, it might not be rational or reasonable for $a$ and $b$ to disagree about how their arguments are related, but that they would do so is nevertheless consistent with the fact that reasoners often tend to display \emph{confirmation bias}, putting more weight on evidence in support of previous beliefs.

Importantly, we can now reject the model presented in Figure \ref{fig:wrong}. The final AF (3), in particular, encodes an interpretation that includes the attack $(p,w)$. But agent $a$ disagrees with agent $b$ about the presence of this attack, so it is unwarranted to take it for granted that $(p,q)$ will come to influence the outcome of deliberation. Indeed, in order to even begin talking about the outcome we need first some aggregated view on the semantic meaning of the arguments involved, based on the agents' views rather than actual utterances. It seems to us that the function of deliberation is to generate such views, and in the next section we develop a logical framework based on this idea.

\section{Argumentative deliberation: A formalization using modal logic}\label{sec:ddl}

Given a basis which encodes agents' views of the arguments, we are interested in the possible ways agents may deliberate, and how deliberation can create new interpretations. In short, we want to study the \emph{effect} of deliberation on semantic meaning.

Towards formalization, we first define the set of all possible interpretations that may result. These will be the states of our models, and we will represent them using AFs. At this stage there is only one requirement that we require such states to meet, namely that they are all based on the views of the agents. This is encoded in the following definition.
\begin{definition}\label{def:dstate}
Given an argumentative state $\views$, we say that $q \subseteq \Pi \times \Pi$ is a \emph{deliberative state} for $\views$ if
\begin{equation}\label{eq:ds}
\bigcap_{a \in \agents}V_a \subseteq q \subseteq \bigcup_{a \in \agents}V_a
\end{equation}
We collect all deliberative states for $\views$ in the set $\dstate \views$. 
\end{definition}

The requirement that states must be based on the agents' views means that we do not allow deliberation to result in states that deviate from interpretations that are held unanimously by the agents. If everyone agrees on the meaning of an argument, the argument has this meaning no matter how deliberation proceeds.\footnote{Interestingly, this does not mean that a possible unanimity regarding the semantic status of an argument is necessarily reflected in the view aggregated by deliberation, not even when all agents reason according to the same semantics. If the agents differ in their account of \emph{why} an argument should be accepted, in particular, deliberation might lead to its rejection. We will formalize a scenario like this in Section \ref{sec:examples}, Example \ref{ex:ad}.} Having defined the set of states, we can now define the notion of a \emph{deliberative model}.

\begin{definition}\label{def:dk}
Given a deliberative state $\views$, a deliberative model for $\views$ is a tuple $(Q,R)$ such that
\begin{itemize}
\item $Q \subseteq \dstate \views$ is a set of deliberative states for $\views$.
\item $R$ is a relation $R \subseteq Q \times Q$.
\end{itemize}
\end{definition}

The idea is that the relation $R$ encodes a process of deliberation based on the views in $\views$. If $(q_1,q_2) \in Q$ the intuition is that there is some event that can take place in the deliberative state $q_1$ so that the aggregated view is updated, taking us to the deliberative state $q_2$. We abstract away from the deliberative events that can induce such a link, but this could be some agent presenting his point of view, or it could be some joint effort, say a vote, to reach a decision about some argument. In this paper, we will keep things simple and leave the exact content of events unspecified.

As an example of a deliberative model, consider the framework in Figure \ref{fig:del1}. Here, the argumentative state is problematic from the point of view of classical logic. In particular, we have $\views \models_\sem \neg \cdia_a x \land \neg \cdia_a \neg x$ under all $\sem$ from Figure \ref{fig:sem}, arising from the fact that in agent $a$'s view, the argument $x$ attacks itself and is not defeated. Hence it cannot be regarded as either true or false without leading to contradiction, and agent $a$ is prevented from reaching any classically sound conclusions about the status of either argument (since he also perceives $x$ to attack $y$). The agent $b$, on the other hand, has the view that $x$ and $y$ are in opposition to each other; If one of them is accepted the other must be rejected and vice versa. But he has no information which suggests choosing one over the other. In particular, we have $\views \models_\sem \cdia_b y \land \cdia_b \neg y$. Hence from his point of view, the semantic status of $x$ and $y$ remains unclear. Through deliberation, however, it is possible to arrive at a definite outcome which also resolves the inconsistency that $a$ believes to be present at $x$. 

One such scenario is depicted in Figure \ref{fig:sem}, where deliberation starts with the empty framework over $\{x,y\}$ and then proceeds by agent $a$ first putting forth his point of view, resulting in $q_1$, and then continuing with agent $b$ adding to this his own understanding, which results in the deliberative state $q_2 = V_a \cup V_b$. Here there is no problem, and the status of $x$ and $y$ has been definitely resolved, since $y$ must be accepted and then $x$ will be defeated, under all semantics from Figure \ref{fig:sem}.

\begin{figure}
$\begin{array}{llllll}
V_a: \xymatrix{& x \ar[r] \ar@(lu,ld) & y}, & V_b: \xymatrix{x \ar@/_/[r] & y \ar@/_/[l] } \\ \\
q_0: \xymatrix{ x & y }, & q_1: \xymatrix{& x \ar@(lu,ld)  \ar[r] & y }, & q_2: \xymatrix{&  x \ar@(lu,ld)  \ar@/_/[r] & y \ar@/_/[l]}
\end{array}$
\caption{A deliberative model $(Q,R)$ over $\views = (V_a,V_b)$ with $R = \{(q_0,q_1),(q_1,q_2)\}$}
\label{fig:del1}
\end{figure}

This is an example of a scenario where everything runs smoothly and there is no controversy. In particular, both agents uncritically accept adding each others' points of view to the aggregated deliberative state, resulting in the union of their views emerging as the final outcome of deliberation. Things might not be so simple, however, and it is the more complicated scenarios that can benefit the most from logical modeling. It could be, for instance, that agent $a$ has reservations about agent $b$'s interpretation of $y$ as an argument that also attacks $x$. If we are unsure about agent $a$'s stance in this regard, or, more generally, unsure about whether deliberation based on the views of agents $a$ and $b$ will eventually return a state where the $(y,x)$-attack is included, we can model this by introducing branching in the deliberative model. In particular, we could introduce a reflexive loop at $q_1$, to indicate the possibility that $b$'s perspective might come to be rejected. Then we have a branching deliberative model, and while it is still \emph{possible} to resolve the problems with original argumentative state, deliberation can then also fail to do so. 

To talk about deliberative models, allowing us to distinguish and identify situations such as these, we can use existing modal languages of varying expressive power. We will focus on making conceptual points, so we consider the following simple language $\lang_1$, which adds to $\lblack$ a modality for talking about the current deliberative state and the one-step possibilities in deliberative models.

$$
\phi := \cdia \alpha \ \mid \ \cdia_a \alpha \ \mid \ \neg \phi \ \mid \ \phi \land \phi \ \mid \adia \phi
$$
where $\alpha \in \lang$. We now use $\cdia$ as a modality to talk about the semantic status of arguments at a deliberative states, while the agent-indexed modalities still apply to the agents' views in the argumentative state. The definition of satisfaction for $\lang_1$ on deliberative models is then defined analogously to classical modal logic.

\begin{definition}\label{truth1}
Given an argumentation semantics $\sem$, an argumentative state $\views$ and a corresponding deliberative model $(Q,R)$, the truth of $\phi \in \lang_1$ on $(Q,R)$ at $q \in Q$ is defined inductively as follows (we omit the boolean cases).
\begin{itemize}
\item $\views, (Q,R),q\models_\sem \cdia \alpha$ if $\exists \clab \in \sem(q): \overline \clab(\alpha) = 1$
\item $\views,(Q,R),q\models_\sem \cdia_a \alpha$ if $\exists \clab \in \sem(V_a): \overline \clab(\alpha) = 1$
\item $\views,(Q,R),q \models_\sem \adia \phi$ if there is $q' \in Q$ s.t. $(q,q') \in R$ and $\views,(Q,R),q' \models_\sem \phi$
\end{itemize}
We follow the usual convention of dropping an argument on the left-hand side to signify universal quantification, such that $\models_\sem \phi$ means that for all $\views$, all models $(Q,R)$ for $\views$, and all states $q \in Q$, we have $\views, (Q,R), q \models_\sem \phi$ 
\end{definition}
 
We define $\abox \phi := \neg \adia \neg \phi$ as usual. Consider the model from Figure \ref{fig:del1} as an example. It is easy to verify that $\views, (Q,R), q_0 \models_\sem \abox \abox \cbox \neg x$, expressing how two steps of deliberation will necessarily suffice to resolve $a$'s semantic problems with $x$ in this scenario, leading us to conclude $\neg x$ at the social level. However, if we add a reflexive edge $(q_1,q_1)$ to this model, to encode uncertainty about whether agent $b$'s view will survive deliberation, we obtain only the weaker $\views, (Q,R),q_0 \models_\sem \adia \adia \cbox \neg x$. It is still \emph{possible} that the problems at $x$ are resolved, but this is no longer necessarily so.

This example shows that we can now formally describe situations where deliberation serves to turn individual views that are problematic into deliberative states that are classically consistent. In the next section, we will see some more examples of how our formalism can be used to capture such mechanisms.

\subsection{Using deliberative logic to model argumentative deliberation}\label{sec:examples}

In \cite{mercier}, one of the primary claims concerns confirmation bias, the mechanism by which reasoners disproportionately tend to favor reasons that support previous beliefs rather than challenge them. According to the authors, this bias is not necessarily an example of flawed reasoning since it has an argumentative function that can serve to enhance the positive effects of deliberation. This claim is supported by empirical evidence, and in this section we show how scenarios where cognitive bias plays a constructive role can be represented by deliberative models and reasoned with using modal logic. Following this, we go on to consider some more examples which we believe illustrate that as an approach to modeling, the formal framework suggested in this paper appears to be both flexible and expressive. 

\begin{example}[Rain in Bergen revisited]
We return to the Bergen rain example, considered in depth in Section \ref{ex:run}. There we argued that instead of directly modeling the actual exchange of arguments, the deliberative events that took place, we should start from a representation of the agents' view of the arguments. These were given in Figure \ref{rainview}, and we argued that these views were consistent with the deliberative event under consideration. Now we can try again to model the deliberation that took place, more abstractly by seeing it as a traversal on a deliberative model. To do this, we will consider the possible effect that each utterance could have on the deliberative state, given a starting point where $r$ and $\overline r$ are in recognized mutual opposition to each other. We recall that the first event was that agent $a$ pointed out the weather report, and that the second event was agent $b$ pointing to the puddle. This leads us then naturally to consider the deliberative model $(Q,R)$, depicted in Figure \ref{rain}.
\begin{figure}
$\begin{array}{lll}
Q: & \hspace{0.3cm} & R: \\ 
\begin{array}{ccccccccc}
q_0 && q_1 && q_2 \\ 
\xymatrix{ w & p \\ r \ar@/_/[r] & \bar r \ar@/_/[l] } &&  \xymatrix{ w \ar[d] & p \\ r \ar@/_/[r] & \bar r \ar@/_/[l] } && \xymatrix{ w \ar[d] & p \ar[d] \\ r \ar@/_/[r] & \bar r \ar@/_/[l] } \\ \\
q_3 && q_4 && q_5 \\
\xymatrix{ w \ar[d] \ar[r] & p \ar[d] \\ r \ar@/_/[r] & \bar r \ar@/_/[l] } && \xymatrix{ w \ar[d] & p \ar[d] \ar[l] \\ r \ar@/_/[r] & \bar r \ar@/_/[l] } && \xymatrix{ w \ar@/_/[r] \ar[d]  & p \ar[d] \ar@/_/[l] \\ r \ar@/_/[r] & \bar r \ar@/_/[l] } \\ \\
\end{array} &\hspace{0.3cm} & 
\begin{array}{l} \\
\xymatrix{ & q_2 & q_4 \\
q_0 \ar[r] & q_1 \ar[ur] \ar[u] \ar[d] \ar[dr] \\ & q_3 & q_5 }
\end{array}
\end{array}$
\caption{The deliberative model $(Q,R)$ based on the argumentative state $\views$ defined in Figure \ref{rainview}}
\label{rain}
\end{figure}

For an intuitive presentation of the scenario, we assume that $(Q,R)$ describes a situation where deliberation is based on searching for agreement about what attacks \emph{not} to include. That is, unless the agents agree on something else we end in the deliberative state which is formed by taking the union of the agents' views. Hence if disagreement runs deep, the Bergen rain debate will end in state $q_5$. Here it is not hard to see that the question of whether or not it will rain remains unsettled -- both $\{w,\bar r\}$ and $\{r,p\}$ are admissible in the resulting AF. In this case, then, debating only served to establish the social fact that the question of whether it will rain in Bergen is still open in the social group $\{a, b\}$, both can make up his own mind and disagreement may persist. However, if the agents are willing to consider a consensus, then they can settle on either $r$ or $\bar r$, by moving to state $q_3$ or $q_4$ respectively. Moreover, they can also choose to conclude, in agreement, that the available evidence is \emph{insufficient} to draw any conclusion. This in particular, is the outcome resulting from the following deliberative state, which emerges from debate if the agents are prudent and reach agreement on including only those attacks that are present in both views. This means that they end in state $q_2$, and here both $w$ and $p$ are regarded as successful, meaning that \emph{both} $r$ and $\bar r$ becomes defeated and impossible to accept.\footnote{In this paper we only sketch a framework that permits us to logically examine spaces of possible outcomes, such as those identified by $(Q,R)$. We remark, however, that a natural next step is to try to investigate which one of these would actually result from cooperation, given some assumptions about the faculties of the agents involved, and depending on how arbitration takes place inside coalitions.} 

This consensus outcome is interesting since it seems plausible that in an actual debate, this is what one would get if the agents respect each others' input. Hence it might be the outcome we \emph{should} get. After all, it represent both a clear conclusion, and also a ``fair" one in light of the evidence, where neither agent looses on grounds that he believes to be unreasonable. More interesting still, it is a consequence of the model that this outcome is only achievable because the agents display confirmation bias with respect to their own arguments; logically, there is little doubt that the two arguments, pulling in opposite directions, attack each other. Yet from the fact that each agent underestimates his opponent's arguments beyond what is rationally warranted, a situation is created whereby deliberation may result in a non-trivial, reasonable interpretation that produces an unambiguous and fair outcome to end the disagreement regarding whether or not it will rain in Bergen today. There is, as usual in Bergen, no way of knowing.

\end{example}

\begin{example}[Two wrongs that make a right]\label{ex:wr}
Consider the argumentative states depicted below. $$
\begin{array}{llll}
V_a: & \xymatrix{ & v \ar[dr] \ar[dl] \ar@(u,l) \\ w \ar[rr] && u \ar@(ru,rd) } & \hspace{1.5cm} V_b: & \xymatrix{ & v \ar@(u,l) \ar[dr] \\ w \ar[ur]  && u \ar[ll] }
\end{array}
$$

Here the two agents both have an inconsistent view on the semantic elements, as the reader may easily verify. Moreover, the agents agree that $v$ attacks itself. But even so, deliberation can result in consistency being regained. In particular, the AF depicted below is a deliberative state for $(V_a,V_b)$ and it is easily seen to be classically consistent, under the evaluation $\{v \mapsto 0, w \mapsto 1, u \mapsto 0\}$.
$$
\xymatrix{ &  v \ar@(u,l) \ar[dr] \\ w \ar[ur] \ar@/_/[rr] && u \ar@/_/[ll] }
$$
In fact, we can say more about deliberation based on $(V_a,V_b)$. It is true, in particular, that we can evaluate the three arguments classically if, and only if, we end up in a deliberative state where $w$ is understood to attack both $v$ and $u$. How to ensure that deliberation leads to this result remains unclear, but we can recognize it as a possible way in which, for this argumentative state, two wrongs could in fact make it right.

\end{example}

\begin{example}[An agreement that disagrees with itself]\label{ex:ad}
While deliberation can sometimes take us from an inconsistent argumentative state to deliberative states that admit classical evaluation, the direction of deliberation can also go in the other direction. Consider, for instance, the following views
$$
\begin{array}{llll}
V_a: & \xymatrix{ u \ar[r] & v \ar@/_/[dl] \ar[d] \\ w \ar@/_/[ur] \ar[u] & p } &\hspace{1.5cm} V_b: & \xymatrix{u \ar@/_/[d] \ar[r] & v \ar[d] \ar[dl] \\ w \ar@/_/[u] & p }
\end{array}
$$

For $\views = (V_a,V_ b)$ we have $\views \models_\sem \cbox_a (\neg v \land p) \land \cbox_b (\neg v \land p)$. That is, the agents both have an interpretation of the world such that $v$ comes out false and $p$ comes out true under all labellings permitted by any of the semantics from Figure \ref{fig:sem}. Moreover, both views can be evaluated classically, as the reader may easily verify. But the agents disagree about something else, namely the status of $u$ and $w$. In particular, we have $\views \models_\sem \cbox_a (w \land \neg u) \land \cbox_b (\neg w \land u)$. This disagreement could spell disaster for deliberation taking place on the basis of $\views$. It is easy to see, in particular, that the following state $q$ could result through deliberation.
$$
\xymatrix{  u \ar[r] & v \ar[d] \ar[dl] \\ w \ar[u] & p }
$$
In this case there are \emph{no} classical evaluations available. That is, for any deliberative model containing $q$, we have $\views, (Q,R), q \models \neg \cdia x \land \neg \cdia \neg x$ for all $x \in \{o,v,w,p\}$. Hence deliberation, in this case, took unproblematic views and produced a problematic outcome. How to avoid this mechanism in general is a difficult problem that deserves further study, but we can already conclude that our model allows us to capture how deliberation can sometimes end up undermining agreements about the status of arguments, and can also lead to the introduction of \emph{new} inconsistencies. Indeed, this too is a mechanism that is intuitively clear. For instance, if one man happens to be right, for all the right reasons, while everyone else either disagrees with his conclusion or his reasons, then deliberation typically lead to suboptimal outcomes. This is important to remember, all the while the other, more positive potential in deliberation, is essentially just an expression of the same mechanism. The whole is more than the sum of its parts, and in order to explore and evaluate it we need new tools and principles that sees it as such.
\end{example}

\subsection{The search for formal characterizations of rational argumentative interaction}\label{sub:for}

Having set up a logical framework for describing argumentative deliberation, we are now also in a position to begin exploring formal representations of principles of rational argumentative interaction. We can think of such principles as restrictions on the class of deliberative models we consider permissible, allowing us to distinguish the ``good" scenarios from the ``bad" ones in a formally precise way. This can be done semantically, in terms of direct mathematical definition of the appropriate classes of models, or axiomatically, by using formulas and schemata from some formal language to express key properties that we believe can serve to characterize good deliberation.

We have already seen how deliberative logic allows us to model scenarios where the outcome is classically sound even if all individual views are inconsistent. The requirement that deliberation should be organized in such a way that it \emph{always} functions in this way might suggest itself as a good candidate for a normative notion of rationality, defined in terms of social interaction. It is a very strong notion, however, and it is potentially problematic also because it is not in fact wholly social. In particular, a requirement to the effect that the outcome of deliberation should always be classically consistent must by necessity also involve restriction on what individual views we permit agents to endorse. 

This is easy to see intuitively. The case of a system with a single agent who believes something absurd, for instance, or a system with many agents where all of them share an inconsistent interpretation of the world are obvious examples. The fact that deliberation alone cannot ensure a consistent outcome in such cases seems clear, and it is an insight that we can now express formally. First we must define an appropriate formalization of classical soundness at the level of deliberation, and how to do this is not obvious. We will discuss some subtleties regarding this later, but for now let us simply consider the following intuitive axiom schema for classically rational deliberation.
\begin{equation}\label{crat}
\cdia \neg (\phi \leftrightarrow \neg \phi)
\end{equation}
If we require it to be true on all models, in all points, we stipulate that for all claims we can express about the model, it should always be possible to reason about this claim in such a way that it is not considered to be equivalent to its own negation. The restriction is perhaps too weak, but at least it appears like a reasonable minimal requirement. We note, moreover, that the scheme is \emph{not} valid on the class of all deliberative models. Hence it captures a non-trivial principle, a genuine restriction on deliberation. However, we also notice that for some argumentative states $\views$, there are \emph{no} corresponding deliberative models such that Schema \ref{crat} holds. Therefore, if we impose it as an axiom of deliberation, we also restrict the class of permissible argumentative states, meaning that it does not provide a purely deliberative approach to rationality. Hence we have a formal counterpart to the intuition that constraints on deliberation alone is not enough to ensure classical consistency in all circumstances.

This motivates the definition of two different kinds of deliberative rationality principles, which can help us to provide more structure to future inquiries.

\begin{itemize}
\item {\bf Liberal principles:} Rationality constraints that do not force us to restrict the set of argumentative states that we consider possible. 
\item {\bf Idealistic principles:} Rationality constraints that require us to restrict the set of possible argumentative states.
\end{itemize} 

An example of a liberal principle could for instance be $\neg \abox (\neg p \land p)$, expressing seriality of deliberative models. In the context of deliberation it would be the principle of that deliberation is open-ended, that there is always a deliberative next step (although at some point it might just be an endless repetition of previously visited states). A more subtle example, involving deliberative interactions, is the principle $\cdia \phi \to \abox \cdia \phi$, expressing that if something is true in a deliberative state it should also be true in all following states. This is not merely a restriction on the kinds of relations we may used in deliberative models, but also a restriction on how deliberation is allowed to unfold from the argumentative state. It is easy to see that it is liberal, however, since a single state without successors will always satisfy it. Notice that if we add a loop to such a state, it witnesses to the liberality of the principle which requires seriality plus commitment to previous outcomes; a debate that never ends and can only increase the set of acceptable truths.

For an example of an idealistic principle, notice that Schema \ref{crat} is idealistic since it excludes certain argumentative states. In fact, we can provide a simple characterization of those argumentative states that are permitted. Let us say that an argumentative state $\views$ satisfies an axiom schema if there is some deliberative model based on this argumentative state for which the schema is true in all states. Moreover, let us say that an argumentative state $\views = (V_a)_{a \in \agents}$ is finite if $\Pi(V_a)$ is finite for all $a \in \agents$. Then we have the following result.

\begin{theorem}\label{thm:1}
For all semantics $\sem$ from Figure \ref{fig:sem}, a finite argumentative state $\views = (V_a)_{a \in \agents}$ satisfies Schema \ref{crat} if, and only if, there is some deliberative state $q$ for $\views$ which admits $\clab \in \sem(q)$ such that $\forall x \in \Pi: \clab(x) \in \{1,0\}$.
\end{theorem}

\begin{proof}
$\Leftarrow$) Immediate from Equation \ref{eq:lsem}. \\
$\Rightarrow$) We let $A = \bigcup_{a \in \agents}\Pi(V_a)$ and form the conjunction $\phi = \band_{x \in A}(x \lor \neg x)$. Then for all assignments $\clab: \Pi \to \three$ we have $\bar \clab(\phi) \in \{1,\frac{1}{2}\}$. We also have $\bar \clab(\phi) = \frac{1}{2}$ if, and only if, there is $x \in A$ such that $\clab(x) = \frac{1}{2}$. Let $q$ be a deliberative state for $\views$ such that $\cdia \neg (\phi \leftrightarrow \neg \phi)$ is true on $q$ under $\sem$. Hence there is $\clab \in \sem(q)$ such that $\clab(x) \in \{1,0\}$ for all $x \in A$. Notice that $\Pi(q) \subseteq A$ by Definition \ref{def:dstate}, so that it follows from Equation \ref{eq:cam} that $\clab(x) = 1$ for all $x \not \in A$. Hence $\clab(x) \in \{1,0\}$ for all $x \in \Pi$.
\end{proof}

This theorem illustrates the kinds of results we can obtain when we begin to formalize deliberative principles using logic. It also suggests the subtleties involved, and how classical concepts might take on new and surprising forms. Notice, for instance, how the case of two wrongs that make a right, considered in Example \ref{ex:wr}, follows from the result. So according to Schema \ref{crat}, such a collection of views will be permitted, even if each view is not in itself classically consistent.

Rather than forbidding the views, we must instead exclude some deliberations that they can give rise to.
How to do this can be a tricky question, and the language of $\lang_1$ might be too weak to deliver principles  that allow us to get very far in this regard. For instance, consider a scenario where deliberation proceeds in a step-wise fashion, such that one argument is considered at a time starting from the deliberative state which contains no attacks. This protocol itself seems perfectly reasonable, but it now becomes highly unreasonable to require classical consistency at every state. It makes more sense to stipulate weaker notions, for instance that classical consistency should \emph{eventually} hold, after deliberation has had a chance to work. In general, the ability to express that something holds eventually is an important addition to the expressive power of a modal language, and in the context of deliberation it allows us to consider a whole range of interesting notions.

In the following we merely sketch some of these, intended to serve as an illustration of the great potential for using stronger modal languages to reason about deliberative models. In particular, we define the modality $\adia^\ast \phi$, intuitively to be read as saying ``after finitely many steps, $\phi$ becomes true''. Formally, we let $\adia^n \phi$ denote $\underbrace{\adia \adia \ldots \adia}_n \phi$ and define satisfaction for $\adia^\ast \phi$ inductively as follows
\begin{equation}\label{sem:star}
\views,(Q,R),q \models_\sem \adia^\ast\phi \text{ if there is } n \in \mathbf N: \views,(q,R),q\models_\sem \adia^n \phi
\end{equation}

We also define $\abox^\ast \phi := \neg \adia^\ast \neg \phi$. This then expresses ``always $\phi$". With these constructs in hand we can express many subtly different properties of deliberation, some of which might be seen as candidates for rationality principles.

Let us assume that $\phi$ expresses some principle which we take to define ``good" states in a normative theory. For instance, $\phi$ could be an instance of Schema \ref{crat}. However, even if we believe that $\phi$ captures some essential normative requirement on the outcome of deliberation, it is not clear that we should require \emph{all} states in a deliberative model to be good states. Indeed, it can often seem more natural to think of deliberation as a process that should ideally take us from bad to good states. Then it is inappropriate, and overly simplistic, to implement a normative ideal by simply forbidding bad states. Instead, we might want to restate our principle $\phi$ in one of the following ways, as a requirement on what it should be possible to achieve through deliberation starting from the current state.

\begin{itemize}
\item $\abox \phi$; all deliberative events take us to a state where $\phi$ is true.
\item $\adia \phi$; there is at least one event taking us to a state where $\phi$ is true.
\item $\adia^\ast \phi$; there is a chain of events such that $\phi$ eventually becomes true.
\item $\adia^\ast \abox \phi$; there is a chain of events taking us to a state where every event will make $\phi$ true.
\item $\adia^\ast \abox^\ast \phi$; there is a chain of events taking us to a state where no further chain of events can make $\phi$ false.
\item $\abox^\ast \adia^\ast \phi$; for every chain of events, there is a way to continue this chain so that $\phi$ eventually becomes true.
\item $\abox^\ast \adia^\ast \abox^\ast \phi$; for every chain of events, there is a continuation so that $\phi$ eventually becomes true, and remains true forever.
\end{itemize}

These are examples of notions that can be formulated using temporal languages. In the the context of classical modal logic they are quite well understood, but when $\phi$ also involves occurrences of modalities such as $\cdia$, expressing argumentative properties of states, new questions arise, both technical and philosophical. One of the most interesting concerns expressive power, and in future work we hope to consider various temporal and fixed-point languages, asking what they allow us to say about deliberative models. Moreover, we think it will be fruitful to combine this technical work with addressing the philosophical challenge of developing a better understanding of the nature of argumentative deliberation. To study normative principles axiomatically is a particularly interesting aspect of this work, and we believe the preliminary investigation carried out here shows its promise.

\section{Conclusion}\label{sec:conc}

We have presented a formal approach to the argumentative theory of reason and argued that it should be studied further, using logical tools. The argumentative theory is interesting because it suggests an alternative approach to rationality, which sees it as irreducibly social and emergent from deliberation. Hence we think it can contribute important insights to the search for new foundations for rationality, foundations that could prove relevant to a range of different fields, including artificial intelligence, which increasingly focuses on social processes and the study of intelligent interaction.

We hope the simple formal framework developed in this paper can provide a point of departure for the development of more sophisticated technical tools. In the first instance, these should be developed so that they allow clear and informative mathematical modeling of important aspects of argumentative deliberation. But we also think that such formalisms can facilitate a highly interesting axiomatic approach to deliberative rationality,  allowing formal logical investigation of foundational issues, assisted by the use of temporal and fixed-point modal languages interpreted on argumentative structures. On the technical side, we think the study of the expressive power of various languages that allow interactions between computational and argumentative modalities is highly interesting and should be considered further. 

The study of deliberative models is the study of how representations of meaning and communicative events together give rise to branching structures of deliberation, where what is truly sound and rational might be hidden somewhere in a combinatorial object, a recurring pattern, or else not emerge at all except in the limit. Wherever it is hiding, and whatever its true nature might be, we believe the pursuit itself has merit, and that it should continue. In the end, perhaps the search for truth is itself a deliberative imperative, the normative power of which is derived not from the fact that it may some day settle, but from the fact that it must always be carried on.

\bibliography{pp}
\bibliographystyle{apalike}
\end{document}

